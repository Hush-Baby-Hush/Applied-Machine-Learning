This is David Forsyth from the University of Illinois. I'm going to talk about the core ideas for evaluating a classifier. So, when I want to evaluate the accuracy of a classifier, what can I do? I could report the false positive rate and the false negative rate. You should understand that a classifier that has a very low false positive rate, i.e, very seldom says, this thing is a one when it's actually a minus one, might very well have a very high false negative rate. So the reason it isn't saying it's a one when it's a minus one is it just says everything is minus one, then it doesn't make any false positives. So when you report a false positive rate you should also report a false negative rate. One sees occasionally reports of one without the other. This is first a bad practice and secondly you should regard this as a justification for assuming that the person who reported that is trying to mislead you. If somebody says well I've got this wonderful classifier, it's got an extremely low false positive rate. And next week when I can do the experiments I'll tell you the false negative rate, they're trying to hide something. Well you can assume they're trying to hide something and you'll probably be right. There are other numbers that you can report. So it's common in medical circles to report the sensitivity which is the percentage of true positives that were classified positive. And the specificity which is the percentage of true negatives classified negative. You should at this point get out a pencil and paper and figure out how those relate to false positive and false negative rate. It's a fairly straightforward sum. If I have a multi-class classifier, just reporting the accuracy doesn't tell me all that much. What I'd really like to do is to report what's known as a class confusion matrix. Now a class confusion matrix is a table and when I put them in the notes, I always put true and predict. Here the columns are organized by the predicted label. The rows are organized by the true label. So the true label goes 0 1 2 3 4 that way, the predicted label goes 0 1 2 3 4, and each cell corresponds to the pair. So for example this cell corresponds to true is zero predict is 0, I got 151 over there. The diagonal fairly obviously corresponds to things that were predicted correctly. Now the reason this is more helpful than accuracy, than just reporting accuracy, is the off diagonal terms tell you something. So over here there were thirty two examples, there should be one but we have predicted zero. I'll put a little arrowhead over there to prevent it from being confusing. Over here there are 13 examples that should be three but were predicted one. Now I've added to this table, as is usual, another little piece of information which is helpful. So for each class 0 1 2 3 4, the error rate, so the percentage of examples in that class that were misclassified, is marked. Now this table is a good example of why a class confusion matrix might be quite helpful, because if you look at this classifier, it is really good at telling whether something is 0 or not. It's about an 8 percent error rate for 0, and then when you look at the other classes, it's completely hopeless at telling the difference between one two three and four. So it knows when it's not 0 and it knows when it's 0, but it doesn't know if it's not 0 whether it's one two three and four. Why? Because the class error rates are very high in the case of four, the class error rate is 100 percent. It simply can't tell you whether something is four, but it knows that it's not a 0. Looking at a table like this is a suggestion that I might do something and the thing that I might do is if I can get away with it, regard the classes one, two, three and four as equivalent, and then build a little binary classifier that is 0 1 1, then I might have to have something more specialized that tells the difference between everything else. It tells the different cases between the everything else class. Okay, here is a fundamental point about errors and is the most important point about classifiers. If you hold on to this point you can usually do no wrong, or at least the wrong you do will be relatively harmless. Training errors are not the same as test errors. Why? We want a classifier that works best at test time. So either it's not the case that the credit card company is going to give me a data set of transactions and say, do well at predicting whether these transactions are fraudulent or not. What's going to happen is they'll give me a data set of transactions and they'll say, use these to predict whether a future transaction, which we don't know because we haven't had it yet, and whose label we will never know is fraudulent, and we want to do well at that task. We haven't got the test data and we usually can't get it and we don't know its labels anyhow, that's why we're trying to build a classifier. Now when I take that training data set, so a bunch of data for example from the credit card company, these are the features, this is false or this is fraudulent or good, and I choose a classifier, that classifier has been chosen to do well on that training set. Why? Because that's at least part of their criteria that I'm going to use. What that means is the error on that training set is better than the error on anything else, which means if I get 99 percent accuracy on the training set and I run around boasting about how accurate my classifier is, I'm going to be in for a shock because when I run it on real data the accuracy should be worse. Now that means when we use those training examples to choose the classifier we can't report the error on those training examples as an estimate of what the test error will be because that estimate will be biased low, it will be wrong. Training error is not the same as test errors. So the classifier has chosen to do well on the training data, so it will likely do better on the training data than on any other data. There are all sorts of names for this term, for this problem. You need to understand the nature of the problem, it's worth remembering the names. One is selection bias, the other is overfitting, the other is overtraining, yet another is overgeneralization, lots of names for the problem. The problem is fundamental. So you cannot evaluate a classifier on data that you use to train it because you will get the wrong answer. The wrong answer will often be optimistic. It will make you look good but it will be wrong. Never evaluate on training data. Now that creates a problem for me, and the problem is, I want to know how well my classifier works but I'm not allowed to evaluate on training data, what am I going to do? Well there's a natural procedure: split the training data into two pieces. I will call one train and one test. Annoyingly every time I split the training data which I will do several times, I might call one piece train and test, and it's a bit difficult to keep track of which is which, that's life. You train using the training piece and I could do anything I like with that piece. Then I'd take the test piece which I have not touched which is clean, it hasn't been used to train the classifier, and I evaluate the classifier using the test piece by the very simple procedure of comparing the classifier's prediction to the right answer. Now I know the right answer because that was part of my original data set. There is a problem here and the problem is the training set is too small. I haven't used all of my data to train the classifier because I kept some of the data out to form a test set. The second problem is that the test set is too small as well, because if I didn't keep out a small piece of training data, I would have made my training data set even more too small. So here is an idea to resolve this problem which is extremely important, it's known as cross-validation. What I could do is take the data set and split it. Then I train on the training part and I test on the test part. Then I do that again with a different random split. And what I'm going to do is I'm going to average the accuracy over the splits. So why am I averaging the accuracy? Because now I have seen a larger test data set which was not used in training. So each time I split, I train a classifier on the one side, I test on the other side, that test is an estimate of accuracy. If I do multiple such things and average, I get a better estimate of accuracy. The most common form of this is known as leave-one-out cross-validation, where you use n minus one data items to train and one to test, and you just do that repeatedly. Now this used to be something nobody did and the reason it used to be something nobody did was first computers either didn't exist or weren't cheap. Now really all this involves computation and you do it. The nice thing about cross-validation in general is it yields an unbiased estimate of error. The estimate of error is accurate. And the reason it's accurate is you are testing the classifier on data that it hasn't seen in training and whose labels you know. So that presumably looks like the test data. Of course this isn't going to help you if the test data doesn't look like the training data. We're always assuming that the test that looks like the training data. Again in the sense that I'm going to weasel about. So here's the summary for this little movie. Now if you're reading the notes as you should be, you'll recognize that what I did was I cropped out some parts of the PDF and I'm going to read them out to you or summarize them. Why, because you should be reading the book and the point of the movie is to help you read the book. So the first point is a classifier is a procedure that accepts a set of features and produces a label. Classifiers are trained and labeled examples, but the goal is to get something that performs well on data that is not seen at the time of training. You really should remember classifiers perform better on training data than on test data because the classifier was chosen to do well on training data. This is overfitting. To get an accurate estimate of future performance, classifiers should always be evaluated on data that was not used in training. Somebody who tells you the performance of their classifier on training data may very well be trying to mislead you or just completely confuse.