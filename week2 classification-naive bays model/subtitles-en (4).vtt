WEBVTT

1
00:00:00.320 --> 00:00:04.310
This is David Forsythe from the University of Illinois.

2
00:00:04.310 --> 00:00:10.888
I'm going to talk about how you choose a model for a Naive Bayes classifier.

3
00:00:10.888 --> 00:00:14.880
We have two topics to look at now in Naive Bayes.

4
00:00:14.880 --> 00:00:18.600
One is how to choose a model and know how well that model works.

5
00:00:18.600 --> 00:00:20.265
If you think back,

6
00:00:20.265 --> 00:00:22.590
I said I could use one of a variety of

7
00:00:22.590 --> 00:00:27.145
different parametric models to fit the class conditional densities.

8
00:00:27.145 --> 00:00:29.285
Which one should I use?

9
00:00:29.285 --> 00:00:33.730
The general program is going to be I'm going to set up a reasonable family of choices,

10
00:00:33.730 --> 00:00:36.270
I'm going to use the validation set to choose one,

11
00:00:36.270 --> 00:00:38.140
and then I'm going to evaluate the best.

12
00:00:38.140 --> 00:00:40.450
But we need to do that carefully.

13
00:00:40.450 --> 00:00:43.230
The second topic we need to deal with is

14
00:00:43.230 --> 00:00:46.555
Naive Bayes turns out to be rather good at dealing with missing data.

15
00:00:46.555 --> 00:00:49.125
Let me think about choosing a model.

16
00:00:49.125 --> 00:00:53.740
I will set up M models to choose from.

17
00:00:53.740 --> 00:00:56.695
Imagine I've got two-dimensional data,

18
00:00:56.695 --> 00:00:59.500
I might be willing to use either poisson distribution or

19
00:00:59.500 --> 00:01:02.490
normal distribution at each of those two variables.

20
00:01:02.490 --> 00:01:04.780
That means I've got four models.

21
00:01:04.780 --> 00:01:07.535
Which one should I use?

22
00:01:07.535 --> 00:01:10.030
Well, there are some rules to solving this problem.

23
00:01:10.030 --> 00:01:15.110
The first rule is you can't evaluate a model on data you use to fit it.

24
00:01:15.110 --> 00:01:16.645
When I say fit,

25
00:01:16.645 --> 00:01:19.875
I mean, choose the parameters of.

26
00:01:19.875 --> 00:01:23.950
And you can't do that because you've fitted the model to do well on

27
00:01:23.950 --> 00:01:29.510
that particular data so any error estimate is going to be optimistic.

28
00:01:29.510 --> 00:01:34.265
You can't evaluate a model on data that you use to choose it.

29
00:01:34.265 --> 00:01:36.115
Now when I say choose a model, I mean,

30
00:01:36.115 --> 00:01:40.345
which one of those M models should I use.

31
00:01:40.345 --> 00:01:43.585
The reason you can't evaluate the model on data you used to choose it

32
00:01:43.585 --> 00:01:48.895
is because you chose the model to do well on that data.

33
00:01:48.895 --> 00:01:52.390
That means that your estimate of error is going to be

34
00:01:52.390 --> 00:01:56.070
optimistic and it may not do as well on other data.

35
00:01:56.070 --> 00:01:59.590
Finally, you want the best possible estimate of the model parameters,

36
00:01:59.590 --> 00:02:01.270
and by model parameters I mean,

37
00:02:01.270 --> 00:02:03.670
if I'm thinking about poisson versus normal,

38
00:02:03.670 --> 00:02:06.590
I'm thinking about standard deviation and mean

39
00:02:06.590 --> 00:02:10.425
of the normal versus the intensity of the poisson distribution.

40
00:02:10.425 --> 00:02:13.015
I want the best possible estimate for obvious reasons,

41
00:02:13.015 --> 00:02:17.380
but getting that best possible estimate requires some care.

42
00:02:17.380 --> 00:02:20.305
Here's my strategy; I'm going to take

43
00:02:20.305 --> 00:02:22.900
the original labeled data that I want to

44
00:02:22.900 --> 00:02:25.975
use to train the model and I'm going to split it into two sets.

45
00:02:25.975 --> 00:02:30.010
I would call one set the training set and the other set the test set,

46
00:02:30.010 --> 00:02:33.055
and I'll set the test set aside and do nothing with it for the moment.

47
00:02:33.055 --> 00:02:38.920
Then I'm going to take that training set and for each type of model,

48
00:02:38.920 --> 00:02:41.780
I'm going to compute the cross-validated error.

49
00:02:41.780 --> 00:02:46.370
I'll do that by taking that trained piece and splitting it into two pieces.

50
00:02:46.370 --> 00:02:49.135
One of them will be trained and the other will be validated.

51
00:02:49.135 --> 00:02:53.600
Okay, we now got two trains, life is tough.

52
00:02:53.600 --> 00:02:55.340
I'm going to fit on that train,

53
00:02:55.340 --> 00:02:56.915
evaluate on the validate,

54
00:02:56.915 --> 00:03:00.170
and repeat averaging the error.

55
00:03:00.170 --> 00:03:01.905
Why do I do this?

56
00:03:01.905 --> 00:03:04.700
Well, if I do only one evaluation,

57
00:03:04.700 --> 00:03:09.693
then I'm evaluating on too small a validation set.

58
00:03:09.693 --> 00:03:11.330
If I do multiple,

59
00:03:11.330 --> 00:03:14.090
I'll get to see slightly different versions of

60
00:03:14.090 --> 00:03:17.350
the model evaluated on slightly different test sets.

61
00:03:17.350 --> 00:03:19.160
So, now I have an average which is

62
00:03:19.160 --> 00:03:24.350
an unbiased estimate of the performance of each type of model.

63
00:03:24.350 --> 00:03:30.480
Now, what I'm going to do is use that cross-validated error to choose the type of model.

64
00:03:30.480 --> 00:03:33.520
I could choose the model with the best error.

65
00:03:33.520 --> 00:03:38.360
I could choose a model with desirable properties and a low error.

66
00:03:38.360 --> 00:03:43.990
I should notice that I know the variance of the error as well.

67
00:03:43.990 --> 00:03:47.870
So, imagine I have two models,

68
00:03:47.870 --> 00:03:55.067
here's model one, here's model two.

69
00:03:55.067 --> 00:04:03.845
The error for the first one with one standard deviation bars looks like that,

70
00:04:03.845 --> 00:04:08.850
the error for the second one with one standard deviation bars looks like that.

71
00:04:08.850 --> 00:04:10.530
It's pretty hard to get excited by

72
00:04:10.530 --> 00:04:12.915
the difference between these two models in terms of error

73
00:04:12.915 --> 00:04:21.105
because this one is often better,

74
00:04:21.105 --> 00:04:22.855
but is often worse than that one,

75
00:04:22.855 --> 00:04:24.630
they're probably doing about the same thing.

76
00:04:24.630 --> 00:04:26.805
So I can use that, get that variance involved.

77
00:04:26.805 --> 00:04:30.050
In this case, it really doesn't matter which one I choose.

78
00:04:30.050 --> 00:04:34.145
In the green case,

79
00:04:34.145 --> 00:04:39.280
the difference between the models is really substantial,

80
00:04:39.280 --> 00:04:41.985
and I'd really like to have model two.

81
00:04:41.985 --> 00:04:44.190
So I chose the model,

82
00:04:44.190 --> 00:04:48.500
now what I'm going to do is take that model with that type of model,

83
00:04:48.500 --> 00:04:54.445
and I'm going to re-estimate its parameters on the whole of the training set.

84
00:04:54.445 --> 00:04:57.640
Why? Because previously, I got estimates of

85
00:04:57.640 --> 00:05:01.717
error but my estimates of the model's parameters were not that great,

86
00:05:01.717 --> 00:05:03.880
and the reason they're not that great is they didn't use all of

87
00:05:03.880 --> 00:05:07.905
the training data set because I had to use some of it to validate.

88
00:05:07.905 --> 00:05:11.195
You don't have the best estimate for

89
00:05:11.195 --> 00:05:14.260
cross-validation because each estimate omits some data.

90
00:05:14.260 --> 00:05:19.325
So now, I take these best model parameters and I've got the best type of model,

91
00:05:19.325 --> 00:05:23.600
I've got its parameters, and I evaluate that finally on the test set.

92
00:05:23.600 --> 00:05:26.580
Now, there are several things you should notice about this strategy.

93
00:05:26.580 --> 00:05:29.640
It seems elaborate, but it's actually just a couple of lines of code.

94
00:05:29.640 --> 00:05:32.840
You've got some loops and you're in business.

95
00:05:32.840 --> 00:05:35.590
It didn't break the rules so I

96
00:05:35.590 --> 00:05:39.990
used unbiased estimates of error to choose a type of model.

97
00:05:39.990 --> 00:05:43.310
I got the best possible parameters for that type of model,

98
00:05:43.310 --> 00:05:47.605
and I've got an unbiased estimate of its accuracy, so I'm happy.

99
00:05:47.605 --> 00:05:50.932
You should remember this because we're going to use it again and

100
00:05:50.932 --> 00:05:54.100
we're going to use this recipe again.

101
00:05:54.100 --> 00:05:57.045
Now, let me talk about missing data.

102
00:05:57.045 --> 00:05:59.145
Naive Bayes can be rather good about missing data.

103
00:05:59.145 --> 00:06:06.460
Imagine I have a training vector XR that has some missing entries,

104
00:06:06.460 --> 00:06:14.085
one, two, question mark for question mark 6.

105
00:06:14.085 --> 00:06:19.350
Now, if these entries went missing purely as a result of random accidents, for example,

106
00:06:19.350 --> 00:06:21.780
I went up to the park, I wrote some notes,

107
00:06:21.780 --> 00:06:25.765
it rained on the way home, and the raindrops hit this entry and that one.

108
00:06:25.765 --> 00:06:28.925
I may very well be able to get over the problem.

109
00:06:28.925 --> 00:06:31.950
And the way I would get over the problem is relatively straightforward.

110
00:06:31.950 --> 00:06:37.890
Imagine I am fitting a normal distribution to the task

111
00:06:37.890 --> 00:06:40.155
conditional density for the third dimension of

112
00:06:40.155 --> 00:06:44.670
X. I've got some missing values over here, how am I going to do that?

113
00:06:44.670 --> 00:06:46.860
Well, I'm going to estimate the mean of

114
00:06:46.860 --> 00:06:49.173
the normal distribution using only the observed data.

115
00:06:49.173 --> 00:06:51.919
I'll just average the observed data and on

116
00:06:51.919 --> 00:06:54.950
estimated standard deviation using only the observed data.

117
00:06:54.950 --> 00:07:00.030
Again, just like the observed data into the equation and get the number out of it.

118
00:07:00.030 --> 00:07:02.155
Now, think about evaluating the model.

119
00:07:02.155 --> 00:07:05.620
If I have some query data item,

120
00:07:05.620 --> 00:07:10.120
let me do the query in blue because I can change colors,

121
00:07:10.120 --> 00:07:19.605
which happens to have a missing component which might happen.

122
00:07:19.605 --> 00:07:21.540
Then notice what I have to do.

123
00:07:21.540 --> 00:07:24.005
I have to compute this number for

124
00:07:24.005 --> 00:07:27.575
each class and choose the class with the largest value of that number.

125
00:07:27.575 --> 00:07:31.314
What I could do is assume that this data,

126
00:07:31.314 --> 00:07:36.500
the first value is missing entirely as a result of random accidents,

127
00:07:36.500 --> 00:07:38.810
and if it is missing as a result of random accidents,

128
00:07:38.810 --> 00:07:41.490
then I'm just going to sum over the second class,

129
00:07:41.490 --> 00:07:45.335
the third class and the fourth class and ignore that class.

130
00:07:45.335 --> 00:07:47.390
Very often this works out.

131
00:07:47.390 --> 00:07:49.180
You need to be careful here.

132
00:07:49.180 --> 00:07:52.495
The procedure is okay if data is missing at random,

133
00:07:52.495 --> 00:07:54.035
but if it's missing for a reason,

134
00:07:54.035 --> 00:07:55.970
you may have very serious problems.

135
00:07:55.970 --> 00:07:57.560
Here's the extreme example.

136
00:07:57.560 --> 00:07:58.885
You're a biologist.

137
00:07:58.885 --> 00:08:01.490
For some reason, you want to classify sleep-deprived versus

138
00:08:01.490 --> 00:08:04.700
normal rats using two features,

139
00:08:04.700 --> 00:08:07.765
their weight and their tail length.

140
00:08:07.765 --> 00:08:10.195
Now, it turns out sleep-deprived rats bite.

141
00:08:10.195 --> 00:08:14.940
Any rat-handler will tell you this so tail length measurements are really hard to get.

142
00:08:14.940 --> 00:08:18.120
In fact, your technician

143
00:08:18.120 --> 00:08:21.495
measured only the weight of the sleep-deprived rats which is easy to do,

144
00:08:21.495 --> 00:08:23.970
you just stick him on a scale in a box that

145
00:08:23.970 --> 00:08:27.175
didn't reach in and handle the rat because I didn't want to get bitten.

146
00:08:27.175 --> 00:08:30.630
In that case, the data is missing for a reason,

147
00:08:30.630 --> 00:08:32.720
and the fact that it's missing tells you something,

148
00:08:32.720 --> 00:08:36.615
and then you can't get away with what I described. But very often you can.

149
00:08:36.615 --> 00:08:40.793
Okay, we're at a summary. Naive Bayes classifiers

150
00:08:40.793 --> 00:08:43.380
are straightforward to build and very effective.

151
00:08:43.380 --> 00:08:45.490
It's easy to deal with missing data.

152
00:08:45.490 --> 00:08:50.865
Experience has shown that Naive Bayes has a particularly effective high dimensional data.

153
00:08:50.865 --> 00:08:54.245
They're easy to build, they work.

154
00:08:54.245 --> 00:08:58.130
We've been through an important recipe for selecting models.

155
00:08:58.130 --> 00:09:00.430
You need to remember that recipe because I'm going to use it again.

156
00:09:00.430 --> 00:09:03.148
If you don't understand, go back in the video and read the notes.

157
00:09:03.148 --> 00:09:05.350
There is a problem point here that is worth

158
00:09:05.350 --> 00:09:08.050
remembering and that causes a lot of irritation to a lot of

159
00:09:08.050 --> 00:09:10.150
people which is you can build

160
00:09:10.150 --> 00:09:14.425
a good Naive Bayes classifier with a bad class conditional model.

161
00:09:14.425 --> 00:09:19.590
Sometimes, weaker class conditional models were built better classifiers.

162
00:09:19.590 --> 00:09:24.070
That's why we have to worry about selecting models and that's it for Naive Bayes.