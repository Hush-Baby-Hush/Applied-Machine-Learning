This is David Forsythe from the University of Illinois. I'm going to talk about how you choose a model for a Naive Bayes classifier. We have two topics to look at now in Naive Bayes. One is how to choose a model and know how well that model works. If you think back, I said I could use one of a variety of different parametric models to fit the class conditional densities. Which one should I use? The general program is going to be I'm going to set up a reasonable family of choices, I'm going to use the validation set to choose one, and then I'm going to evaluate the best. But we need to do that carefully. The second topic we need to deal with is Naive Bayes turns out to be rather good at dealing with missing data. Let me think about choosing a model. I will set up M models to choose from. Imagine I've got two-dimensional data, I might be willing to use either poisson distribution or normal distribution at each of those two variables. That means I've got four models. Which one should I use? Well, there are some rules to solving this problem. The first rule is you can't evaluate a model on data you use to fit it. When I say fit, I mean, choose the parameters of. And you can't do that because you've fitted the model to do well on that particular data so any error estimate is going to be optimistic. You can't evaluate a model on data that you use to choose it. Now when I say choose a model, I mean, which one of those M models should I use. The reason you can't evaluate the model on data you used to choose it is because you chose the model to do well on that data. That means that your estimate of error is going to be optimistic and it may not do as well on other data. Finally, you want the best possible estimate of the model parameters, and by model parameters I mean, if I'm thinking about poisson versus normal, I'm thinking about standard deviation and mean of the normal versus the intensity of the poisson distribution. I want the best possible estimate for obvious reasons, but getting that best possible estimate requires some care. Here's my strategy; I'm going to take the original labeled data that I want to use to train the model and I'm going to split it into two sets. I would call one set the training set and the other set the test set, and I'll set the test set aside and do nothing with it for the moment. Then I'm going to take that training set and for each type of model, I'm going to compute the cross-validated error. I'll do that by taking that trained piece and splitting it into two pieces. One of them will be trained and the other will be validated. Okay, we now got two trains, life is tough. I'm going to fit on that train, evaluate on the validate, and repeat averaging the error. Why do I do this? Well, if I do only one evaluation, then I'm evaluating on too small a validation set. If I do multiple, I'll get to see slightly different versions of the model evaluated on slightly different test sets. So, now I have an average which is an unbiased estimate of the performance of each type of model. Now, what I'm going to do is use that cross-validated error to choose the type of model. I could choose the model with the best error. I could choose a model with desirable properties and a low error. I should notice that I know the variance of the error as well. So, imagine I have two models, here's model one, here's model two. The error for the first one with one standard deviation bars looks like that, the error for the second one with one standard deviation bars looks like that. It's pretty hard to get excited by the difference between these two models in terms of error because this one is often better, but is often worse than that one, they're probably doing about the same thing. So I can use that, get that variance involved. In this case, it really doesn't matter which one I choose. In the green case, the difference between the models is really substantial, and I'd really like to have model two. So I chose the model, now what I'm going to do is take that model with that type of model, and I'm going to re-estimate its parameters on the whole of the training set. Why? Because previously, I got estimates of error but my estimates of the model's parameters were not that great, and the reason they're not that great is they didn't use all of the training data set because I had to use some of it to validate. You don't have the best estimate for cross-validation because each estimate omits some data. So now, I take these best model parameters and I've got the best type of model, I've got its parameters, and I evaluate that finally on the test set. Now, there are several things you should notice about this strategy. It seems elaborate, but it's actually just a couple of lines of code. You've got some loops and you're in business. It didn't break the rules so I used unbiased estimates of error to choose a type of model. I got the best possible parameters for that type of model, and I've got an unbiased estimate of its accuracy, so I'm happy. You should remember this because we're going to use it again and we're going to use this recipe again. Now, let me talk about missing data. Naive Bayes can be rather good about missing data. Imagine I have a training vector XR that has some missing entries, one, two, question mark for question mark 6. Now, if these entries went missing purely as a result of random accidents, for example, I went up to the park, I wrote some notes, it rained on the way home, and the raindrops hit this entry and that one. I may very well be able to get over the problem. And the way I would get over the problem is relatively straightforward. Imagine I am fitting a normal distribution to the task conditional density for the third dimension of X. I've got some missing values over here, how am I going to do that? Well, I'm going to estimate the mean of the normal distribution using only the observed data. I'll just average the observed data and on estimated standard deviation using only the observed data. Again, just like the observed data into the equation and get the number out of it. Now, think about evaluating the model. If I have some query data item, let me do the query in blue because I can change colors, which happens to have a missing component which might happen. Then notice what I have to do. I have to compute this number for each class and choose the class with the largest value of that number. What I could do is assume that this data, the first value is missing entirely as a result of random accidents, and if it is missing as a result of random accidents, then I'm just going to sum over the second class, the third class and the fourth class and ignore that class. Very often this works out. You need to be careful here. The procedure is okay if data is missing at random, but if it's missing for a reason, you may have very serious problems. Here's the extreme example. You're a biologist. For some reason, you want to classify sleep-deprived versus normal rats using two features, their weight and their tail length. Now, it turns out sleep-deprived rats bite. Any rat-handler will tell you this so tail length measurements are really hard to get. In fact, your technician measured only the weight of the sleep-deprived rats which is easy to do, you just stick him on a scale in a box that didn't reach in and handle the rat because I didn't want to get bitten. In that case, the data is missing for a reason, and the fact that it's missing tells you something, and then you can't get away with what I described. But very often you can. Okay, we're at a summary. Naive Bayes classifiers are straightforward to build and very effective. It's easy to deal with missing data. Experience has shown that Naive Bayes has a particularly effective high dimensional data. They're easy to build, they work. We've been through an important recipe for selecting models. You need to remember that recipe because I'm going to use it again. If you don't understand, go back in the video and read the notes. There is a problem point here that is worth remembering and that causes a lot of irritation to a lot of people which is you can build a good Naive Bayes classifier with a bad class conditional model. Sometimes, weaker class conditional models were built better classifiers. That's why we have to worry about selecting models and that's it for Naive Bayes.