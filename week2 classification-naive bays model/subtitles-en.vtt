WEBVTT

1
00:00:00.000 --> 00:00:02.610
This is David Forsyth from the University of Illinois.

2
00:00:02.610 --> 00:00:08.210
I'm going to talk about the core ideas for evaluating a classifier.

3
00:00:08.210 --> 00:00:11.860
So, when I want to evaluate the accuracy of a classifier, what can I do?

4
00:00:11.860 --> 00:00:17.075
I could report the false positive rate and the false negative rate.

5
00:00:17.075 --> 00:00:23.930
You should understand that a classifier that has a very low false positive rate, i.e,

6
00:00:23.930 --> 00:00:32.000
very seldom says, this thing is a one when it's actually a minus one,

7
00:00:32.000 --> 00:00:38.365
might very well have a very high false negative rate.

8
00:00:38.365 --> 00:00:40.810
So the reason it isn't saying it's a one when it's

9
00:00:40.810 --> 00:00:43.785
a minus one is it just says everything is minus one,

10
00:00:43.785 --> 00:00:47.105
then it doesn't make any false positives.

11
00:00:47.105 --> 00:00:52.085
So when you report a false positive rate you should also report a false negative rate.

12
00:00:52.085 --> 00:00:57.410
One sees occasionally reports of one without the other.

13
00:00:57.410 --> 00:01:01.850
This is first a bad practice and secondly you should regard this as

14
00:01:01.850 --> 00:01:06.925
a justification for assuming that the person who reported that is trying to mislead you.

15
00:01:06.925 --> 00:01:09.580
If somebody says well I've got this wonderful classifier,

16
00:01:09.580 --> 00:01:12.715
it's got an extremely low false positive rate.

17
00:01:12.715 --> 00:01:17.753
And next week when I can do the experiments I'll tell you the false negative rate,

18
00:01:17.753 --> 00:01:19.340
they're trying to hide something.

19
00:01:19.340 --> 00:01:22.595
Well you can assume they're trying to hide something and you'll probably be right.

20
00:01:22.595 --> 00:01:24.860
There are other numbers that you can report.

21
00:01:24.860 --> 00:01:28.645
So it's common in medical circles to report

22
00:01:28.645 --> 00:01:34.075
the sensitivity which is the percentage of true positives that were classified positive.

23
00:01:34.075 --> 00:01:38.380
And the specificity which is the percentage of true negatives classified negative.

24
00:01:38.380 --> 00:01:40.785
You should at this point get out a pencil and paper

25
00:01:40.785 --> 00:01:44.525
and figure out how those relate to false positive and false negative rate.

26
00:01:44.525 --> 00:01:50.130
It's a fairly straightforward sum. If I have a multi-class classifier,

27
00:01:50.130 --> 00:01:53.575
just reporting the accuracy doesn't tell me all that much.

28
00:01:53.575 --> 00:01:58.270
What I'd really like to do is to report what's known as a class confusion matrix.

29
00:01:58.270 --> 00:02:03.270
Now a class confusion matrix is a table and when I put them in the notes,

30
00:02:03.270 --> 00:02:05.020
I always put true and predict.

31
00:02:05.020 --> 00:02:09.575
Here the columns are organized by the predicted label.

32
00:02:09.575 --> 00:02:12.530
The rows are organized by the true label.

33
00:02:12.530 --> 00:02:16.965
So the true label goes 0 1 2 3 4 that way,

34
00:02:16.965 --> 00:02:20.575
the predicted label goes 0 1 2 3 4,

35
00:02:20.575 --> 00:02:23.220
and each cell corresponds to the pair.

36
00:02:23.220 --> 00:02:29.300
So for example this cell corresponds to true is zero predict is 0,

37
00:02:29.300 --> 00:02:31.600
I got 151 over there.

38
00:02:31.600 --> 00:02:37.910
The diagonal fairly obviously corresponds to things that were predicted correctly.

39
00:02:37.910 --> 00:02:39.965
Now the reason this is more helpful than accuracy,

40
00:02:39.965 --> 00:02:41.810
than just reporting accuracy,

41
00:02:41.810 --> 00:02:44.235
is the off diagonal terms tell you something.

42
00:02:44.235 --> 00:02:47.780
So over here there were thirty two examples,

43
00:02:47.780 --> 00:02:51.530
there should be one but we have predicted zero.

44
00:02:51.530 --> 00:02:54.710
I'll put a little arrowhead over there to prevent it from being confusing.

45
00:02:54.710 --> 00:03:01.425
Over here there are 13 examples that should be three but were predicted one.

46
00:03:01.425 --> 00:03:02.885
Now I've added to this table,

47
00:03:02.885 --> 00:03:07.705
as is usual, another little piece of information which is helpful.

48
00:03:07.705 --> 00:03:11.835
So for each class 0 1 2 3 4,

49
00:03:11.835 --> 00:03:14.580
the error rate, so the percentage of examples in

50
00:03:14.580 --> 00:03:18.745
that class that were misclassified, is marked.

51
00:03:18.745 --> 00:03:23.325
Now this table is a good example of why a class confusion matrix might be quite helpful,

52
00:03:23.325 --> 00:03:25.295
because if you look at this classifier,

53
00:03:25.295 --> 00:03:30.900
it is really good at telling whether something is 0 or not.

54
00:03:30.900 --> 00:03:33.150
It's about an 8 percent error rate for 0,

55
00:03:33.150 --> 00:03:36.105
and then when you look at the other classes,

56
00:03:36.105 --> 00:03:40.515
it's completely hopeless at telling the difference between one two three and four.

57
00:03:40.515 --> 00:03:43.380
So it knows when it's not 0 and it knows when it's 0,

58
00:03:43.380 --> 00:03:47.575
but it doesn't know if it's not 0 whether it's one two three and four.

59
00:03:47.575 --> 00:03:51.510
Why? Because the class error rates are very high in the case of four,

60
00:03:51.510 --> 00:03:53.040
the class error rate is 100 percent.

61
00:03:53.040 --> 00:03:56.155
It simply can't tell you whether something is four,

62
00:03:56.155 --> 00:03:58.870
but it knows that it's not a 0.

63
00:03:58.870 --> 00:04:02.850
Looking at a table like this is a suggestion that I might do something and

64
00:04:02.850 --> 00:04:06.055
the thing that I might do is if I can get away with it,

65
00:04:06.055 --> 00:04:07.560
regard the classes one, two,

66
00:04:07.560 --> 00:04:09.345
three and four as equivalent,

67
00:04:09.345 --> 00:04:13.090
and then build a little binary classifier that is 0 1 1,

68
00:04:13.090 --> 00:04:15.660
then I might have to have something more specialized that tells

69
00:04:15.660 --> 00:04:18.315
the difference between everything else.

70
00:04:18.315 --> 00:04:22.610
It tells the different cases between the everything else class.

71
00:04:22.610 --> 00:04:25.420
Okay, here is a fundamental point

72
00:04:25.420 --> 00:04:27.955
about errors and is the most important point about classifiers.

73
00:04:27.955 --> 00:04:32.575
If you hold on to this point you can usually do no wrong,

74
00:04:32.575 --> 00:04:35.905
or at least the wrong you do will be relatively harmless.

75
00:04:35.905 --> 00:04:39.300
Training errors are not the same as test errors.

76
00:04:39.300 --> 00:04:44.155
Why? We want a classifier that works best at test time.

77
00:04:44.155 --> 00:04:46.850
So either it's not the case that

78
00:04:46.850 --> 00:04:51.350
the credit card company is going to give me a data set of transactions and say,

79
00:04:51.350 --> 00:04:56.375
do well at predicting whether these transactions are fraudulent or not.

80
00:04:56.375 --> 00:04:59.795
What's going to happen is they'll give me a data set of transactions and they'll say,

81
00:04:59.795 --> 00:05:04.055
use these to predict whether a future transaction,

82
00:05:04.055 --> 00:05:07.085
which we don't know because we haven't had it yet,

83
00:05:07.085 --> 00:05:11.065
and whose label we will never know is fraudulent,

84
00:05:11.065 --> 00:05:13.765
and we want to do well at that task.

85
00:05:13.765 --> 00:05:15.590
We haven't got the test data and we

86
00:05:15.590 --> 00:05:17.640
usually can't get it and we don't know its labels anyhow,

87
00:05:17.640 --> 00:05:19.950
that's why we're trying to build a classifier.

88
00:05:19.950 --> 00:05:22.955
Now when I take that training data set,

89
00:05:22.955 --> 00:05:26.945
so a bunch of data for example from the credit card company,

90
00:05:26.945 --> 00:05:28.400
these are the features,

91
00:05:28.400 --> 00:05:32.660
this is false or this is fraudulent or good,

92
00:05:32.660 --> 00:05:34.745
and I choose a classifier,

93
00:05:34.745 --> 00:05:39.890
that classifier has been chosen to do well on that training set.

94
00:05:39.890 --> 00:05:44.090
Why? Because that's at least part of their criteria that I'm going to use.

95
00:05:44.090 --> 00:05:46.795
What that means is the error on

96
00:05:46.795 --> 00:05:51.905
that training set is better than the error on anything else,

97
00:05:51.905 --> 00:05:55.740
which means if I get 99 percent accuracy on

98
00:05:55.740 --> 00:05:59.780
the training set and I run around boasting about how accurate my classifier is,

99
00:05:59.780 --> 00:06:02.780
I'm going to be in for a shock because when I run it on

100
00:06:02.780 --> 00:06:06.740
real data the accuracy should be worse.

101
00:06:06.740 --> 00:06:11.520
Now that means when we use those training examples to

102
00:06:11.520 --> 00:06:15.525
choose the classifier we can't report

103
00:06:15.525 --> 00:06:20.010
the error on those training examples as an estimate of what

104
00:06:20.010 --> 00:06:25.290
the test error will be because that estimate will be biased low, it will be wrong.

105
00:06:25.290 --> 00:06:28.135
Training error is not the same as test errors.

106
00:06:28.135 --> 00:06:31.590
So the classifier has chosen to do well on the training data,

107
00:06:31.590 --> 00:06:35.880
so it will likely do better on the training data than on any other data.

108
00:06:35.880 --> 00:06:39.285
There are all sorts of names for this term, for this problem.

109
00:06:39.285 --> 00:06:40.969
You need to understand the nature of the problem,

110
00:06:40.969 --> 00:06:42.420
it's worth remembering the names.

111
00:06:42.420 --> 00:06:43.675
One is selection bias,

112
00:06:43.675 --> 00:06:44.820
the other is overfitting,

113
00:06:44.820 --> 00:06:46.406
the other is overtraining,

114
00:06:46.406 --> 00:06:48.045
yet another is overgeneralization,

115
00:06:48.045 --> 00:06:50.790
lots of names for the problem. The problem is fundamental.

116
00:06:50.790 --> 00:06:54.255
So you cannot evaluate a classifier

117
00:06:54.255 --> 00:06:57.975
on data that you use to train it because you will get the wrong answer.

118
00:06:57.975 --> 00:06:59.970
The wrong answer will often be optimistic.

119
00:06:59.970 --> 00:07:02.440
It will make you look good but it will be wrong.

120
00:07:02.440 --> 00:07:05.255
Never evaluate on training data.

121
00:07:05.255 --> 00:07:07.942
Now that creates a problem for me,

122
00:07:07.942 --> 00:07:09.140
and the problem is,

123
00:07:09.140 --> 00:07:10.650
I want to know how well my classifier

124
00:07:10.650 --> 00:07:13.126
works but I'm not allowed to evaluate on training data,

125
00:07:13.126 --> 00:07:14.620
what am I going to do?

126
00:07:14.620 --> 00:07:18.725
Well there's a natural procedure: split the training data into two pieces.

127
00:07:18.725 --> 00:07:21.430
I will call one train and one test.

128
00:07:21.430 --> 00:07:25.310
Annoyingly every time I split the training data which I will do several times,

129
00:07:25.310 --> 00:07:27.220
I might call one piece train and test,

130
00:07:27.220 --> 00:07:30.030
and it's a bit difficult to keep track of which is which, that's life.

131
00:07:30.030 --> 00:07:35.455
You train using the training piece and I could do anything I like with that piece.

132
00:07:35.455 --> 00:07:40.857
Then I'd take the test piece which I have not touched which is clean,

133
00:07:40.857 --> 00:07:42.799
it hasn't been used to train the classifier,

134
00:07:42.799 --> 00:07:46.795
and I evaluate the classifier using the test piece

135
00:07:46.795 --> 00:07:48.850
by the very simple procedure of

136
00:07:48.850 --> 00:07:52.215
comparing the classifier's prediction to the right answer.

137
00:07:52.215 --> 00:07:56.270
Now I know the right answer because that was part of my original data set.

138
00:07:56.270 --> 00:08:01.460
There is a problem here and the problem is the training set is too small.

139
00:08:01.460 --> 00:08:04.360
I haven't used all of my data to train the classifier because I

140
00:08:04.360 --> 00:08:09.055
kept some of the data out to form a test set.

141
00:08:09.055 --> 00:08:12.040
The second problem is that the test set is too small as well,

142
00:08:12.040 --> 00:08:14.145
because if I didn't keep out a small piece of training data,

143
00:08:14.145 --> 00:08:17.735
I would have made my training data set even more too small.

144
00:08:17.735 --> 00:08:21.325
So here is an idea to resolve this problem which is extremely important,

145
00:08:21.325 --> 00:08:23.225
it's known as cross-validation.

146
00:08:23.225 --> 00:08:30.005
What I could do is take the data set and split it.

147
00:08:30.005 --> 00:08:34.625
Then I train on the training part and I test on the test part.

148
00:08:34.625 --> 00:08:39.840
Then I do that again with a different random split.

149
00:08:39.840 --> 00:08:45.180
And what I'm going to do is I'm going to average the accuracy over the splits.

150
00:08:45.180 --> 00:08:49.405
So why am I averaging the accuracy?

151
00:08:49.405 --> 00:08:55.705
Because now I have seen a larger test data set which was not used in training.

152
00:08:55.705 --> 00:08:57.075
So each time I split,

153
00:08:57.075 --> 00:08:58.875
I train a classifier on the one side,

154
00:08:58.875 --> 00:09:00.465
I test on the other side,

155
00:09:00.465 --> 00:09:02.890
that test is an estimate of accuracy.

156
00:09:02.890 --> 00:09:06.660
If I do multiple such things and average,

157
00:09:06.660 --> 00:09:10.110
I get a better estimate of accuracy.

158
00:09:10.110 --> 00:09:14.505
The most common form of this is known as leave-one-out cross-validation,

159
00:09:14.505 --> 00:09:20.275
where you use n minus one data items to train and one to test,

160
00:09:20.275 --> 00:09:21.880
and you just do that repeatedly.

161
00:09:21.880 --> 00:09:23.470
Now this used to be something nobody did

162
00:09:23.470 --> 00:09:25.180
and the reason it used to be something nobody did

163
00:09:25.180 --> 00:09:29.995
was first computers either didn't exist or weren't cheap.

164
00:09:29.995 --> 00:09:34.595
Now really all this involves computation and you do it.

165
00:09:34.595 --> 00:09:36.620
The nice thing about

166
00:09:36.620 --> 00:09:40.035
cross-validation in general is it yields an unbiased estimate of error.

167
00:09:40.035 --> 00:09:42.620
The estimate of error is accurate.

168
00:09:42.620 --> 00:09:45.380
And the reason it's accurate is you are testing

169
00:09:45.380 --> 00:09:49.520
the classifier on data that it hasn't seen in training and whose labels you know.

170
00:09:49.520 --> 00:09:52.095
So that presumably looks like the test data.

171
00:09:52.095 --> 00:09:53.720
Of course this isn't going to help you if

172
00:09:53.720 --> 00:09:55.680
the test data doesn't look like the training data.

173
00:09:55.680 --> 00:09:58.645
We're always assuming that the test that looks like the training data.

174
00:09:58.645 --> 00:10:00.780
Again in the sense that I'm going to weasel about.

175
00:10:00.780 --> 00:10:02.710
So here's the summary for this little movie.

176
00:10:02.710 --> 00:10:04.625
Now if you're reading the notes as you should be,

177
00:10:04.625 --> 00:10:07.550
you'll recognize that what I did was I cropped out some parts

178
00:10:07.550 --> 00:10:10.940
of the PDF and I'm going to read them out to you or summarize them.

179
00:10:10.940 --> 00:10:12.920
Why, because you should be reading the book and the point of

180
00:10:12.920 --> 00:10:16.625
the movie is to help you read the book.

181
00:10:16.625 --> 00:10:18.560
So the first point is a classifier is

182
00:10:18.560 --> 00:10:21.780
a procedure that accepts a set of features and produces a label.

183
00:10:21.780 --> 00:10:24.162
Classifiers are trained and labeled examples,

184
00:10:24.162 --> 00:10:26.180
but the goal is to get something that performs well

185
00:10:26.180 --> 00:10:29.060
on data that is not seen at the time of training.

186
00:10:29.060 --> 00:10:33.760
You really should remember classifiers perform better on training data than on

187
00:10:33.760 --> 00:10:36.250
test data because the classifier was

188
00:10:36.250 --> 00:10:39.775
chosen to do well on training data. This is overfitting.

189
00:10:39.775 --> 00:10:42.438
To get an accurate estimate of future performance,

190
00:10:42.438 --> 00:10:46.990
classifiers should always be evaluated on data that was not used in training.

191
00:10:46.990 --> 00:10:52.045
Somebody who tells you the performance of their classifier on

192
00:10:52.045 --> 00:10:57.250
training data may very well be trying to mislead you or just completely confuse.