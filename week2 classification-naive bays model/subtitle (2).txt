I'm David Forsyth, from the University of Illinois at Urbana-Champaign. I'm going to talk about Nearest Neighbors classifiers. This is our first real classifier as opposed to just sort of talking about classifiers generically. What I'm going to do is assume that features are a tuple of fixed size, and that I can measure distances between tuples. This usually means that features are really vectors of fixed dimensions, so, my features in the credit card case might involve something like, where the transaction was made, how much money is involved, where the previous transaction was made, how much money was involved, and that a variety of other numbers organized into a vector. So, I've got a procedure, and the procedure is very simple. I will have a dataset, which will consist of traditionally N pairs. Why? Because, every dataset consists of N pairs, and it turns out every feature vector has dimension D. Each pair consists of a feature vector with dimension D and a label. The label is the Y_1 through Y N, the feature vector is the X_1 through X N. So, let me name them. Notice, that I did not say anything about what the label was. So, now, I have a new feature vector X, and I would like to predict its label. You can think of this as a test item. For some reason, people who do Nearest Neighbors a lot, call it a query. So, I'm going to take this test item, and what I'm going to do for Nearest Neighbors is I will find the example that is closest to the query, in the distance, and I will simply report that example's label. So, I take my X over to here, find the closest example, pick out its label, and that's Y closest. That is the Nearest Neighbors classifier. Here is a second algorithm, which is also quite effective, and in fact sometimes better. Instead of finding the closest example, I will find the K closest examples. K is usually an odd number, but it doesn't absolutely have to be, and then I will vote on those labels. So, I'll take those labels, and I'll report the most common. That is known as K Nearest Neighbors. There's a variant of K Nearest Neighbors, which is when I vote, if the label with the largest number of votes has fewer than our votes, then I say I've no idea what this is, that's known as K R Nearest Neighbors. I don't think I've ever seen anybody do that, but I've seen it described in any number of places, so I'll mention it. Here's an example, so, remember our alien who wanted to classify people by gender, by looking at their height, I have marked a whole bunch of examples, and I have separated these examples. So, let me change colors. I've marked a whole bunch of examples on our graph here. This is height in inches, I have separated these examples vertically, purely for my own convenience in drawing. This doesn't mean anything. The female heights are down here, the male heights are down there. This is from real data. So, now my alien gets an example, a query example, over here, shall we say, and chooses the closest data point. Now, the closest data point is a little hard to identify from the drawing, because I'd need to run a perfectly straight, vertical line, which is palpably beyond my powers, but it looks like this one. So, for that query the alien would report male. Now, notice for another query that's very close by, the closest example would be that one, and the alien would report female. So, in this range, from about here to about here, the alien would sometimes report male, and sometimes report female. More females than males over here. More males than females over there, and then if I'm over here, the alien is pretty much guaranteed to report male, and over there, it's pretty much guaranteed to report female. So, here's a neat thing about Nearest Neighbors that's being displayed by this drawing. It really is quite responsive to the scatter of the data. When the features are highly ambiguous, then the answer tends to change a lot. You make a tiny change in the feature, and the label of the nearest thing changes. When the features are profoundly unambiguous, you get the answer right, or at least you get the answer that agrees with the unambiguous features, which one hopes is right. There's some really significant virtues to Nearest Neighbors. The labels can be anything. I didn't say anything about what the labels were. The example I did, they were male and female, but they really can be anything. Nearest Neighbors usually gets better with more data. Now, gets better, by gets better, I mean more accurate. This does depend on the dimension of the data, and that is important. To get really good Nearest Neighbors with high dimensions requires completely implausible quantities of data, unless you have a nice problem. Nearest Neighbors is in fact, can be very well-behaved in practice. A bunch of problems seem to be nice, and you can show that with enough data the error rate is twice the base error. This, doing this formally requires a bit of a performance. If you recognize informal mathematics when it's being done, you should have noticed that what I did in the previous example is a sketch that would put you in roughly the right position to understand why this might be. There are some real problems with Nearest Neighbors as well. One problem, many people don't understand just how hard this problem can be, is it can be very difficult to find the nearest neighbor of a point in a high dimensional space. This is an odd problem, and it's odd because your intuition will say, a tree gets involved, and the whole thing becomes logarithmic, and intuition has said that to people for something like 40 years now, and it's been wrong. It turns out finding the exact nearest neighbor in a high dimensional space, about the best available algorithm is essentially take your query point, check its distance to the first example, now check its distance to the second example, now check its distance to the third example, and keep the example with the smallest distance. Surprisingly, if you are willing to deal with what's known as an approximate nearest neighbor, i.e. find a point that with high probability is almost as close as the nearest neighbor, the problem becomes much easier and you can do it much much, more quickly. I'll talk about that in a couple of slides. A second vice is that the metric matters and it can be hard to estimate. So, one example that shows that the metric might be a real issue is poorly scaled features. Imagine my alien wishes to improve his classifier, or its classifier of human gender, and adds to the feature a second feature, which is the measure of the length of the thumbnail in millimeters. Now, your thumbnail runs to probably somewhere between 7 and 15 millimeters or so. The height is measured in inches which is about 70 and it may very well be the case that the poor scaling between the features means that one or the other of the features is always ignored. If I have a number that goes in the range as the heights do, from about 50 to about 75, the distance in that number is going to be quite large. If I have a number that, like fingernails, runs from about 9 to about 14, the distances in those numbers are going to be quite small. So, the nearest neighbor is going to be completely dominated by the measurement of thumbnail length. And of course that doesn't tell you very much about gender, so, you have a problem. Now, if you understand something about your problem, you may actually be able to re-scale the features, but if you know nothing, it might be very difficult. Good performance of Nearest Neighbors can be quite hard in high dimensions, and it can require very large datasets, which may or may not be available. Now the solution to the difficulty with finding the Nearest Neighbors is to adopt an approximate Nearest Neighbors algorithm. Most current nearest neighbor classifiers actually find the approximate nearest neighbor, that's a point that is almost as close as the nearest neighbor with high probability, and it turns out that that point is just fine for classification purposes. There are two strategies for finding that point. One, is you build a KD tree, and you use a form of limited backtracking. The other is you use a hashing strategy, and it turns out what is attractive about these methods is you can auto tune them on a dataset. So, you have a dataset of N examples, you can take the query method and you can search over a variety of different parameters to find the instance of the query method, that gives you the best behavior for that particular dataset. This can be extremely useful if you're going to run a lot of queries. There is great software available, a callout for David Lowe's FLANN software, which is what I like for approximate Nearest Neighbors and that is very useful for reasonably sized problems. Here's a worked example. Again, you'll notice a cut out from the test, classifying using Nearest Neighbors. Build a nearest neighbor classifier to classify the MNIST digit data. The MNIST data is a dataset that's very widely used to check simple methods. It's a bunch of hand-written digits with the correct label. It's split into a test and a training set. You can find it in several places, and it's comprehensively explored in the exercises. I used the Kaggle version simply because I was lazy. So, here's what happened. I went to R, and R has nearest neighbor code that seems quite good. I haven't had any problems with it. I didn't want to use FLANN. There isn't really very much to say. The test examples or queries, the training examples, the training dataset, you tell the R code, which is which. I trained on a thousand of the 42,000 examples. Why do I do that? I did that because it was idle, and because I recognized that a query against 42,000 examples was going to be a problem. And then I tested on the next 200 examples. If you were doing a full scale experiment, you'd train on all 42,000 examples, and you'd test on all, I think, 10,000 test examples. For my rather small case, I found a class confusion matrix as shown. Remember the meaning of this matrix. So, for example, this cell is the digits that have true label six and where I predicted the label two, and there aren't any of those. Big thing about the class confusion matrix is the big numbers are on the diagonal. There are very few big numbers anywhere else. It works rather well. I didn't put the class error rates in, because I couldn't remember the magic line of R when I did the worked example, that's your problem. As you'll see, there are exercises about MNIST, and you can do a bigger version. So, here's the summary. Nearest neighbor classifiers are often very effective. They can predict any kind of label. You do need to be careful to have enough data, and to have a meaningful distance function.