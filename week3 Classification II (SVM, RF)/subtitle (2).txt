This is David Forsyth from the University of Illinois and I'm going to talk about stochastic gradient descent. What I want to do is to think about how I'm going to minimize that cost function. So, there is a very general recipe for minimizing functions. Assume you have a point u. Choose a direction p, and it's a good idea for that direction to go downhill for fairly obvious reasons, and then move a small amount in that direction, update my point by saying the ith u plus a little bit times that p is the i+1th, and then just iterate that. What is the direction? It's usually downhill. It's either the gradient, or if you remember Newton's method, it's a combination of the gradient and the Hessian. What is h? Well, one thing you could do is, if you have a p that looks promising, search along that p to find an h that is good. Typically by looking at the value where you end up and then choose that h by looking at the different values you've looked at. h is sometimes called the step length. Now the problem we have right now is this story actually doesn't work for the problem we have to solve. Why? Well, let's look at the cost function. The cost function is an average of hinge losses over examples plus this regularization term. Now this is long and annoying as an expression. I want it to be shorter to make the point, so I'm going to write all these terms with a g. My average is an average of gi's, and then there's a g0 which corresponds to this a. That's enough to get me where I want to be. Now think about the gradient. The way you take the gradient is you average the gradients of each of the gi's, and then you add the gradient to g0. That sounds hunky-dory, and it is, unless you've got 100 million gis, at which point it's kind of annoying. So the problem we have to deal with is this n might be enormous. 100 million is not outrageous here, and in the exercises we'll be dealing with hundreds of thousands as sort of a routine exercise. Now, what are we going to do about this? I do not want to average the gradient at 100 million examples every time I take a step. So, notice that that term is actually a population mean. I'm averaging over 100 million the gradient in each example. How do we deal with population means when we can't go and look at everything? Well, we draw a sample and we estimate it with a sample mean. So, what I'm going to do is draw uniformly and at random usually without replacement, but n is so big that it really doesn't matter. A sample of Nb. Nb is a batch size. Typically we choose this batch size because it makes the cache work best, or the disk seek work best, or the GPU happy or something like that. It's a number chosen for practical reasons. I'm going to choose Nb examples and I'm just going to compute their gradients. I'm going to average over those and I'll say that's the direction I'm going to move in. Now, my update is going to be I had some point, u is now a and b stacked together. Why? Because I like writing one equation for a and one equation for b. I've got my new u is the old one plus some amount times the search direction, and my search direction is going to be backwards along a sample mean of gradients, plus the gradient with the penalty. Why is the sample mean a sensible thing to do? Well, firstly you should notice that it's random. So, this vector is random, but I'm not just barging around randomly in u space because the expected value of this is the right thing. If you can't prove that instantly to yourself as we speak, stop the movie at this point, take out a pencil and a piece of paper and prove it. It's pretty obvious, so I'm not going to do it. Now the other thing you have to notice is that searching for a step length is a very bad idea as well. Why? Because to know if I've taken a big enough step, I have to evaluate the cost function. To evaluate the cost function, I have to average 100 million examples, and I don't feel like doing that because it might take weeks. So, I'm not going to do that. Instead, I'm simply going to say take large steps at the start because you need to make big changes, and take small steps near the end. So, eta will start being big and get smaller and smaller. How do I choose that? Well, I break up my training into epochs or seasons or whatever. So, I do a whole bunch of rounds of training and then I update at counter e. My step length for my new round of training is going to look like a constant over that counter plus another constant. Now, you should notice that this expression has some nice features. One is when e is small, this number is big. When e is big, eta is small, which is sort of what you want. The rules here are that eta should limit to zero eventually, and also that the sum of all etas should limit to infinity. It turns out that this rule will have that property, but there are other ones as well. A sensible question here at this point is how do I choose m and n. The usual thing is you try a couple of rounds of training and see what happens. Another thing you should notice over here is I cannot diagnose convergence. I can't tell when my optimization has finished. The reason I can't tell when my optimization is finished is that doing so would involve evaluating the cost function and that's too expansive. So usually what I do is a fixed number of steps, or possibly what I can do is hold out some examples. For them, build a little plot that looks like this: number of steps, error on held out, and that little part is going to wiggle a bit because I'm taking somewhat random steps, but I hope it goes down. I sit there and I'll watch it. If it gets small enough and I'm happy with where it is, I stop it. This algorithm is known as stochastic gradient descent. It's an extremely powerful algorithmic recipe. We will use again.