This is David Forsyth from the University of Illinois and we're going to do the linear support vector machine in this movie. So we need a classification rule and I'm going to make the apparently arbitrary, and it may even seem to you unwise decision, to choose a classifier of the following form. I have binary data. There are n pairs of data items, x_i is a feature vector, y_i is the ith class label. And I will assume that y_i is either 1 or -1. Then to predict the sign of y_i for any point x, we use a linear classifier. And for any new data item, I will predict the sign of a transpose x plus B. Now, of course the issue here is choosing a and b such that we get good behavior on future data. Now the first thing to think about is, what does the rule mean? So here's a really easy example, I've got an alien trying to tell males from females. The alien looks at height, the alien can see there's feature x, the height and it predicts male or female as sign of a_x plus B. What is it doing? Well basically there's some point minus b over a. If the height is bigger than that point it makes one prediction, if the height is smaller than that point it makes another prediction. So, on this side, a times height plus b is positive, on that side a times height plus b is negative or the other way round and I'll map those to male and female and I'm done. So here's the second version, more interesting, the alien now has two features. So it's a two dimensional feature vector x and the alien is looking at a transpose x plus b. Now the rules of linear algebra tell me that a is two dimensional as well. The sign changes where a transpose x plus b equals zero. You should know that this is a line. x is a two dimensional variable. I can think about my 2D vector x as a little x coordinate and a little y coordinate. And if you expand this out, you're going to get an equation that looks like something times x plus something times y plus a constant equals zero. You can check that that makes a line. So my alien now, operating in two dimensions, has a bunch of data. Let's write pluses for one category, minuses for the other category, what my alien is doing is choosing a line that separates those two categories. In this case plus on that side and minus on that side. Now what should occur to you is two concerns which basically occur to everybody at this stage. One is, this linear model might very well seem to you to be too simple. Why? Because if you expend a little imagination on it, you can easily draw datasets where a linear model isn't going to help you very much. Your intuition is misleading you there and I'll draw some pictures now. In fact, a strategy that says if it's not working, make the feature vector bigger, is very good. There are some possible constructions but typically you just add features and I'll show you an example in a second. The next question you could ask is, look, this is a really simple model. Why do you expect it to work well? And the answer is, I don't have to estimate a lot of complicated parameters so I'm more likely to get accurate values. Let's go over the linear model is too simple example, our question. So let me go back to my alien. As you recall, the alien was trying to tell male from female thinking about height. So I'll draw that axis for the height variable and we've got the examples in one dimension, so he's got a bunch of female examples over here. It turns out that human females tend to be less tall than human males, so we sort of expect and mostly would see data distributions that look a bit like this. The taller ones tend to be male or the males tend to be taller, the females tend to be shorter but you're not guaranteed, there are some short males and some tall females. What my alien could do is say, let me add a feature. Now for that, we want a second drawing. The alien, being a well-informed sort of alien as aliens are, adds another relatively simple feature and the feature is the number of y chromosomes. So here is height and here is the number of y chromosomes which mostly is either zero or one. There's some other odd cases but we'll ignore those. The deal here is that the males always have one y chromosome and they tend to be a bit tall, so I'll put them over here, and the females always have zero y chromosomes. Again we're leaving out some genetically slightly odd cases. Usually it's not a particularly good idea to have more than one or fewer than zero y chromosomes and if you do I'm afraid I'm not talking about you. Now, my alien has no problem at all. By inserting one feature, he can choose or, the alien can choose a line that separates these two populations just fine thank you. For example that line. Now this principle operates more generally. Here's another possible example that might worry you. Many people come up with this one. Here's a two dimensional data set that would be really difficult, where it would be really difficult to separate the pluses and the minuses with a line. Now what am I going to do about that? The sensible answer always is add more features. So what I could do is add a simple feature and the simple feature is going to be distance of a data point from the origin. Now we could plot the new three dimensional dataset or I suppose you could visualize it without a plot. But let me try and plot it because I think it's drawable. What am I going to see? Well my pluses are close to the origin so they're going to be low in this new future. My minuses however, are rather far from the origin. So they're going to be up here floating around on some large radius plane. And I should be able to separate these two by a plane that looks at the distance from the origin. So mostly, if you're in trouble with a linear rule like this if you add features, you'll get out of trouble. So we're just not going to worry about this question of the rule being too simple because it isn't. That brings us to a small piece of terminology. A support vector machine is a linear classifier trained with the hinge loss. When I say linear classifier I mean a classifier that does sign of a transpose x plus b. They're usually called SVMs. There are a variety of other things you can do that are also called SVMs but predominantly, that, trained with a hinge loss, is an SVM.