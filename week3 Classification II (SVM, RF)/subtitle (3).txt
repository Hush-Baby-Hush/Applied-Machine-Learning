This is David Forsyth from the University of Illinois, and I'm going to start talking about choosing the regularization constant for an SVM. Now, I need to choose the regularization constant lambda. Remember I said I didn't know the number, but this is just like the choice of modeling Naive Bayes. Remember in Naive Bayes we had a situation where we had several different models, we had to figure out which one we wanted to use. Here we have several different possible values of lambda, we want to figure out which one to use. So, there are some rules. Remember from Naive Bayes, I wanted to choose one of M models, these models have different values of the regularization constant. Many years of experimental work suggest that performance is not particularly twitchy about the value of the regularization constant. So, typically you might look at values like 0.1, 1, 10 or 0.01, 0.1, 1 or something of that sort. What's the big point here? The big point is changing the regularization constant by a factor of about 10 will usually get you some action, changing it by adding 2 percent isn't going to change anything at all which is really convenient. So, I'll set up 10 or 20 different values changing by decades or maybe factors of 3 if I'm nervous and then I'll look at each different one. That forms my M models. Now, the rules are I can't evaluate a model on data I use to fit it, because I fitted the model to do well on that data so the value will be wrong. I can't evaluate a model on data I used to choose it because I chose the model to do well on that data. And finally, I want the best possible estimate of the parameters. So, here's our procedure as in last time. I'm going to split the label data into two sets, I'm going to call them train and test and I will set test aside. I'm not going to do anything with it for a minute. Now, I will go to the train and I will compute cross-validated error so I will split that into train and validate. I will fit on train for a fixed value of lambda and then I will evaluate and validate and I'll repeat. I will do that for each of my different values of lambda. Now, I have a cross-validated error for each value of lambda and it might look like this. 0.1, 1, 10, 0.01. Over here I will have somewhat higher cross-validated error and I'm going to draw in an error bulb because I can. Why? Because I didn't regularize enough. Here I might have somewhat lower cross-validated error because I regularized just fine. Here I regularized too much, so my cross-validated error went up and here I'd regularized a lot too much so it went up even more. And I look at that and I say, okay, well in this case pretty obviously the best value is 0.1. Notice because I've got error bars, I could choose the one that has the best value of error, mean error, which would be this. But I could also choose the value which has the best value of the smallest error. I could choose something that had a value of lambda that I liked and a reasonable error and so on. So, I choose lambda. Now, once I have chosen that lambda, I'm going to go back and use the whole training set to fit the best model. I can use the whole training set, and now I have a slightly better estimate of the parameters. I didn't get the best estimate from cross-validation because each cross-validation round emitted some data. And then, because I want to boast about how well my SVM works, I can evaluate this model on the test set. And that's the first time the test set has been touched. So, I didn't break the rules. I chose lambda fairly on data that hadn't been touched by training and I reported performance fairly on data that hadn't been touched in training. So, here are graphs from an example that appears in the text. This is a typo. It should say size of a not size of w. Over here, I have rounds of training. Here is the error reported by the model. You know this is a fairly easy data set. Basically, after a very small amount of training, each one of these models is getting about 80 percent error, a little bit less a little bit more. These are plotted for different values of regularization constant. So, the red plot is 1e minus 7, which is this. And you can see that eventually it does fairly well. The green plot is 1e minus 5. It's all tangled up with the others in the horizontal line. The purple plot is one which is kind of a regularized and it's making errors as a result. You should notice a couple of things from this. In a really easy example, it all settles down fairly quickly. Each step of training does not necessarily make things better. Look, things got worse over there. Why? Because I'm not actually going exactly downhill. I'm going in a random direction. The expected value of that direction is downhill, but the direction I go in is random. What happens is not sensitive to the size the regularization constant. You'll notice that I've got my constants by a factor of 100 over here and not much is changing. You'll notice I've got my constants by factors of 100 over here and still not much is changing. Finally, if the regularization constant is big, then on the whole the model likes very small a. Why? Because it's expensive to have a big a. If the regularization constant is small on the whole the model likes big a. That isn't perfect and there's some suggestion over here that a middle sized regularization constant gets me even bigger a's and that's not that surprising. Okay, so now, what I'd like to be able to do is expand this procedure to do multiclass classification. How would I do it? Well, one thing I could do is if I had for example four class labels, I could say, well that's really a bit vector. 0 0, 0 1, 1 0, 1 1. Then I'm going to build one classifier to predict this bit, one classifier to predict that bit and I should be able to go home happy. The problem with that strategy is it doesn't work. Reliably doesn't work. And the reason that reliably doesn't work is once you've gotten this bit wrong, nothing you can do over here is going to rescue you. It turns out to be a reliably poor strategy. Here is another strategy that is widely used. It's known as One vs One. So, for every pair of classes I train an SVM. So for example again if I have four classes I might train a 1-2 SVM, a 1-3 SVM a, 1-4, a 2-3, a 2-4 and a 3-4. Now, I take my example that I want to classify and I pass it into each of these SVMs. I then ask, which class got the most votes? And I accept the most popular class. For example, if I have an example of class one, with a little bit of luck, this one will class one. This one will say class one, maybe even that one will say class one and these will say either two, three or four. And, I got three ones and I take that. The scale's fairly badly and the reason the scale is very badly is I have to build the number of SVMs that is quadratic in the number of classes which is annoying. The other thing I could do is One vs All. So I could build an SVM that is 1 - (2 3 4), 2 - (1 3 4), 3 - (1 2 3) and finally, 1 - (1 2 3), 2 - (1 3 4), 3 - (1 2 4), and finally 4 - (1 2 3). Then I take my new example and I place it into each classifier. Then I take the number that comes out of the SVM and remember it's going to be positive on one side and negative on the other and I choose the SVM that produces the biggest number for a class. So, for example if I passed my example in and it gives me 0.2 for one 0.5 for two 0.1 for three and 0.1 for four then I'll take the two because I got the biggest number. This is not wholly legitimate because the SVM doesn't know you're going to use the number like that, but it turns out to work rather well. Okay, so here's a summary. A linear SVM is a go to classify. When you have a binary classification problem, the first thing you should do is try a linear SVM. There is a lot of good software for building. It is also straightforward to build multiclass classifiers out of binary classifiers. Any decent SVM package will do this for you. And that's it.