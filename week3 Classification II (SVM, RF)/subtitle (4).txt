This is David Forsyth from the University of Illinois and I'm going to talk about random forests. So here is the decision tree, this is the household robots guide to obstacles. The household robot encounters an obstacle and it tests something. It says does it move or not move? Now let's assume for the moment it moves, and then it asks, does it bite or not bite because it moves? Well, it bites. So then it asks is it furry or not furry? And it isn't furry and that means it's a toddler. Now it should be pretty clear to you that this is a classifier. Depending on your experience with toddlers, it may not be an accurate one, but it's a classifier. There are several ways to think about these classifiers. I will always think about the decisions that are made on the tree as being binary. So you can either go left or right. It should be pretty obvious to you that I could go three ways, whatever. Much of what I discussed can be extended without comment to going three ways but it's just a bit of a performance. The second thing is I can draw these things in several different ways. So on this side, I've drawn the tree as a tree roughly like what you saw on the previous slide and you'll notice that the first test tests the feature y against 0.32 and if y is bigger than 0.32, you go one way, otherwise you go the other way. And then the next test on the one side, test x against -0.58, you go one way if it's bigger, otherwise you go the other way. And finally on the other side, you test x against 1.06 and if it's bigger you go one way, otherwise you go the other way. Now I can draw all this in 2D as a little KD tree. You can see over here this line corresponds to the first decision. And then if y is greater than 0.32, I'm going to test x against 1.06. Depending on which side I fall, I'm in one of these two cells. And if y is less than 0.3 2, I'm going to test x against minus 0. 58 and depending on, which is this line. And depending on which side of the line I fall I'll be in one of these two cells. And you can notice that the decision tree I've drawn over here has some sort of nice properties. If I end up in this cell, the data are mostly x's. So if I walk down this cell and say that the thing is an x, I've got a good chance of getting it right. If I walk down the tree and end up in this cell and say dot, then I've got a good chance of getting it right. Over here mostly they're zeros, over here mostly they are pluses. And the big question for us is how can I come up with such a tree from data? So there is a problem. It may be very hard to get the best tree for training data. Furthermore, that tree may have lots of little leaves. If you think about the tree I showed on the previous slide, you could make it better by splitting some of the leaves even further. But those leaves may have very little data left in them and you may not be absolutely confident that those leaves would apply on test data. So the best tree for training data is probably unattractive. Why? Because it may have lots of small leaves which creates efficiency problems and it may not work well on test data. It may not generalize. So here is a strategy which turns out to be extremely powerful. I'm going to put together a procedure that involves a great deal of randomness and we'll build quite a good tree. Quite good in a sense that we don't need to discuss. It will be moderately accurate and shallow. So for example I could take the extreme step of using only two leaves, meaning there's exactly one decision. If you're on one side, you're one label if, you're on the other side the other label. That is by rather gross analogy called a decision stump. Now what I'm going to do is make lots of these quite good trees and that object I will think of as a random forest. And I can make them on the same training data set and still be sure that they're different because I'm making them with random aspects. So the randomness matters. And then what I'm going to do is average out the randomness and the decisions to come up with a good or maybe even the best decision. So there are two questions. Firstly, how do I come up with a decent tree? And secondly, what do I do once I've got a forest? Now whenever a tree is involved, computer scientists tend to have fairly good and solid and kind of immediate intuitions. So, how do you make a tree? Pretty clearly, I'm going to recursively take the pool of training data, choose some split which tells the difference between left and right. And then, I've got a left pool and a right pool and I'm going to invoke this procedure on those pools. And I can do that recursively but I have to figure out when I'm going to stop. And I'm going to stop if all the data in the pool has the same label. Why? Because then there's no point in splitting it. If the tree is too deep. Why do I want to stop when the tree is too deep? Well, there are two problems if a tree is deep. One is, I may have a tree with an inconvenient number of leaves. The second is, as the tree gets deeper and deeper, the decisions that I make tend to be based on less and less data and as a result, may be less and less accurate and they probably won't work on test data, so I don't want to go too deep. And the the third reason I might stop is if the pool of data is too small. It is probably not a particularly good idea to figure out a split from two data items because you don't know what the rest of the data is going to do. So these are going to be my stopping criteria. How do I choose a split? I will have, as a convention, that the split is always chosen by testing a feature vector component against a threshold. So my data is described by feature vectors and labels. I look at the feature vector, I look at the earth component and I say is it bigger than or smaller than some fixed threshold? If it's bigger you go one way, if it's smaller you go the other way. Now it should be fairly clear that it's quite easy to choose the best threshold by search given I have the right component. What I'm going to do is come up with some figure of merit for the goodness of a split, then I will build a series of thresholds, possibly even a split between every possible meaningful split for that pool of data, and then I will search that set of thresholds and I'll take the one with the best figure of merit. We will talk about how to make that figure of merit in a second. The other question is which component of the feature vector should I test? And a standard and very good solution for this question is I will randomly choose a small subset of the components, typically about root D. So the root of the dimension, that's going to be about the size of the subset. So if I have a 100 dimensional feature, I'll choose 10 components at random, something like 10. Then what I will do is take each one in turn and search for each one for a good threshold. And I'll keep the figure of merit for the best threshold for that component and then I'll just go to the next one in my set of 10 or or root D, and I'll see which of those root D has the best threshold and I'll keep that. Now what is crucial here is we can put together a figure of merit for the best split. And you can see this easily from these drawing. So if you look on this side, this split is really very good. And the reason it's very good is once I know which side of this tree I'm on, left or right, gee, I got that right. Once I know which side of this tree I'm on, I actually know the label accurately. There's a very good chance that this will work for future data as well. So if I'm on the left side I get an o, if I'm on the right side again x. But if you look at this tree, or the same dataset, with this split, this split is completely unhelpful. If I'm on the bottom, I could have an o or an x, they are about the same frequency. If I'm on the top, I could have an o or an x. So what I would really like to do is to have a split such that, if you split the pool of data, it's really easy to tell which label you have depending on which side you're on. I can formalize that. So to formalize that, I need the notion of entropy and I will not develop it in detail here. I'll assume that those who understand it will follow me in detail, that those who don't but want to will read the notes and those who don't will flip through the video confident that somebody knows what to do about all this. The point here is that I can account for how much information is provided by a split. So if you look at these two splits, this one is actually better than that one. And the reason that one is better than two, if I use split one, on the left side, if I end up there, then I know the label is x. And on the right side if I end up there, then I know that the label is zero with probability two thirds, otherwise x. Reason two, if I, if I use split two, if I end up on the left side, I'm going to get an x with two thirds, but if I end up on the right side, it's still 50/50. So the question I have to worry about is how to account for this. And the way I'm going to account for this is I'm going to compute what's known as the information gain, which is the entropy of the label conditioned on having no split minus the entropy of the label conditioned on having a split. And I want this to be as big as possible. So if you're really into entropy, you'll follow me immediately. There's another side with some calculations for those who are not but want to see some more detail. But basically remember that the entropy of a label given information is how many bits I need to supply to know the true value. So we have a procedure for building a tree. So how do I build a forest? Well, I'm going to take a data set of the usual n pairs of Xi which is a feature vector and Yi a label. I'm going to separate the dataset and do a test in a training set. I'm going to train multiple distinct decision trees on the training set. Now these are going to be distinct. And the reason they're going to be distinct is each time I train a split, I'm going to use a random set of components. So every time I train a tree on this dataset, I'm going to get a different tree. There is an alternative. I could do this by allowing different trees to see different training sets, but keeping track of which trees saw which training item. Then when I test, if the tree has seen that training item, I will leave the training item out of evaluation for that tree alone. So how do I classify now I have a forest? I'm going to take test example x and I'm going to pass it down each tree of the forest. Now I have to choose a strategy. I could say every time the example arrives at a leaf, record one vote for the label that occurs most often at the leaf. Now I'm going to choose the label with the most votes. And this strategy is effective, but you might be worried about one thing, which is you could reasonably feel that when the example arrives at a leaf which is 59x and 41 0, you should think about that vote rather differently than when the example arrives at a leaf with a 1000 x's and no zeros in it. And you could take that into account by each time the example arrives at the leaf, you could record the n l votes for each of the labels that occur at that leaf, where n l is the number of times the label appears in the training data. Now you choose the label with the most votes. So what I'm doing now is waiting leaves that are really confident, somewhat higher. Both strategies seem to work. I'm not aware of any obvious evidence that one is better than the other. So here's a very simple example. The details are in the text. What I did was I took the heart data set, there are five labels, zero, one, two, three and four. We've seen this before when I talked about class confusion matrices and in fact, if you look at this class confusion matrix, you've seen the class confusion matrix before. I used the r random forest package, I fed the data into that package and I got a class confusion matrix that looks like one you've seen before. Now you may recall, when I talked about class confusion matrices, I said it's probably not a good idea to distinguish between one, two, three and four, because basically the decision tree can't do it or the random forest can't do it. So what I might do, is just squash labels one, two, three and four into one case which I'll call one and look at that. And if I do that, I get a classifier that's really very well-behaved. The class error for zero is 16%, the error for one is 22%, the overall accuracy is really rather high. Here's a summary. Random forests are straightforward to build and very effective. I have not given you a detailed account of an algorithm, largely because the small number of people who really need to build the algorithm will have to go a little bit further, everybody else is going to use a package. There are very good software implementations freely available. Random forests are extremely nice because they can predict any kind of label. It does not have to be binary classification.