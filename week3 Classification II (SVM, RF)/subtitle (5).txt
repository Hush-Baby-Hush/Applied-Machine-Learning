This is David Forsyth from the University of Illinois. Let's do a sum. Here, split one. Here, split two, and I'm interested in the entropy of a label, condition on no split minus the entropy of the label, condition on split. It's two split one. Okay, so let me compute the entropy of label given no split. Well, given no split P of X is three over five. So H of label condition on no split will be minus three of the five log three of the five, minus two of the five, log, two over five. I computed that on my calculator which for some reason wouldn't do log two so I computed it in knots rather than bits and I got .67. Now, H of label condition on split is more interesting. So, if I have a split, I don't know which side I'm going to go onto. So, I have to think about P of left and P of right. P of left is going to be two over five. Just by I. P of right is three over five and H should label given split is going to be H of label given left times two over five plus H of label given R times three over five. Now all this is simplified a little bit because H of label conditioned on the left is zero. Why? Because if you go left, you know what the label is. And the label is X. So this is zero. And H of label given right is again, fairly easy to compute, that is going to be minus one third log one over three minus two thirds, log two over three. This expression H of label condition on split is going to be .65. So, my total information gain is going to be.02. If you push through the same calculation, again, in nought, because I only had a lin on my calculator, for side two, the information gained is going to be .01, rather than .02 nought. And by I, that seems feeling natural. Side one, if I go left, I'm in really good shape. If I go right, I'm about as good shape as I was. Side two, if I go right, I'm clueless. If I get left, I'm in about as good shape as I would so one looks better than two.