WEBVTT

1
00:00:00.000 --> 00:00:03.570
This is David Forsyth from the University of Illinois and I'm going

2
00:00:03.570 --> 00:00:07.058
to talk about random forests.

3
00:00:07.058 --> 00:00:08.780
So here is the decision tree,

4
00:00:08.780 --> 00:00:12.435
this is the household robots guide to obstacles.

5
00:00:12.435 --> 00:00:16.540
The household robot encounters an obstacle and it tests something.

6
00:00:16.540 --> 00:00:20.175
It says does it move or not move?

7
00:00:20.175 --> 00:00:24.580
Now let's assume for the moment it moves, and then it asks,

8
00:00:24.580 --> 00:00:29.355
does it bite or not bite because it moves?

9
00:00:29.355 --> 00:00:31.715
Well, it bites.

10
00:00:31.715 --> 00:00:35.950
So then it asks is it furry or not furry?

11
00:00:35.950 --> 00:00:39.175
And it isn't furry and that means it's a toddler.

12
00:00:39.175 --> 00:00:43.160
Now it should be pretty clear to you that this is a classifier.

13
00:00:43.160 --> 00:00:45.360
Depending on your experience with toddlers,

14
00:00:45.360 --> 00:00:46.805
it may not be an accurate one,

15
00:00:46.805 --> 00:00:48.630
but it's a classifier.

16
00:00:48.630 --> 00:00:51.375
There are several ways to think about these classifiers.

17
00:00:51.375 --> 00:00:56.560
I will always think about the decisions that are made on the tree as being binary.

18
00:00:56.560 --> 00:00:58.310
So you can either go left or right.

19
00:00:58.310 --> 00:01:01.430
It should be pretty obvious to you that I could go three ways, whatever.

20
00:01:01.430 --> 00:01:03.470
Much of what I discussed can be extended without

21
00:01:03.470 --> 00:01:08.000
comment to going three ways but it's just a bit of a performance.

22
00:01:08.000 --> 00:01:10.920
The second thing is I can draw these things in several different ways.

23
00:01:10.920 --> 00:01:13.280
So on this side,

24
00:01:13.280 --> 00:01:18.230
I've drawn the tree as a tree roughly like what you saw on the previous slide

25
00:01:18.230 --> 00:01:20.990
and you'll notice that the first test tests the feature

26
00:01:20.990 --> 00:01:23.990
y against 0.32 and if y is bigger than 0.32,

27
00:01:23.990 --> 00:01:25.335
you go one way,

28
00:01:25.335 --> 00:01:26.885
otherwise you go the other way.

29
00:01:26.885 --> 00:01:29.300
And then the next test on the one side,

30
00:01:29.300 --> 00:01:32.015
test x against -0.58,

31
00:01:32.015 --> 00:01:34.550
you go one way if it's bigger,

32
00:01:34.550 --> 00:01:36.620
otherwise you go the other way.

33
00:01:36.620 --> 00:01:37.760
And finally on the other side,

34
00:01:37.760 --> 00:01:41.542
you test x against 1.06 and if it's bigger you go one way,

35
00:01:41.542 --> 00:01:43.087
otherwise you go the other way.

36
00:01:43.087 --> 00:01:50.015
Now I can draw all this in 2D as a little KD tree.

37
00:01:50.015 --> 00:01:56.655
You can see over here this line corresponds to the first decision.

38
00:01:56.655 --> 00:02:01.435
And then if y is greater than 0.32,

39
00:02:01.435 --> 00:02:04.240
I'm going to test x against 1.06.

40
00:02:04.240 --> 00:02:07.090
Depending on which side I fall,

41
00:02:07.090 --> 00:02:09.175
I'm in one of these two cells.

42
00:02:09.175 --> 00:02:11.970
And if y is less than 0.3 2,

43
00:02:11.970 --> 00:02:14.725
I'm going to test x against minus 0.

44
00:02:14.725 --> 00:02:16.525
58 and depending on,

45
00:02:16.525 --> 00:02:18.280
which is this line.

46
00:02:18.280 --> 00:02:24.130
And depending on which side of the line I fall I'll be in one of these two cells.

47
00:02:24.130 --> 00:02:26.580
And you can notice that the decision tree I've

48
00:02:26.580 --> 00:02:29.170
drawn over here has some sort of nice properties.

49
00:02:29.170 --> 00:02:31.360
If I end up in this cell,

50
00:02:31.360 --> 00:02:33.390
the data are mostly x's.

51
00:02:33.390 --> 00:02:37.120
So if I walk down this cell and say that the thing is an x,

52
00:02:37.120 --> 00:02:39.075
I've got a good chance of getting it right.

53
00:02:39.075 --> 00:02:42.550
If I walk down the tree and end up in this cell and say dot,

54
00:02:42.550 --> 00:02:44.830
then I've got a good chance of getting it right.

55
00:02:44.830 --> 00:02:48.940
Over here mostly they're zeros,

56
00:02:48.940 --> 00:02:51.985
over here mostly they are pluses.

57
00:02:51.985 --> 00:02:55.600
And the big question for us is how can I come up with

58
00:02:55.600 --> 00:02:59.995
such a tree from data? So there is a problem.

59
00:02:59.995 --> 00:03:03.670
It may be very hard to get the best tree for training data.

60
00:03:03.670 --> 00:03:06.883
Furthermore, that tree may have lots of little leaves.

61
00:03:06.883 --> 00:03:10.510
If you think about the tree I showed on the previous slide,

62
00:03:10.510 --> 00:03:15.420
you could make it better by splitting some of the leaves even further.

63
00:03:15.420 --> 00:03:19.660
But those leaves may have very little data left in them and you may not

64
00:03:19.660 --> 00:03:23.975
be absolutely confident that those leaves would apply on test data.

65
00:03:23.975 --> 00:03:26.880
So the best tree for training data is probably unattractive.

66
00:03:26.880 --> 00:03:29.380
Why? Because it may have lots of small leaves which

67
00:03:29.380 --> 00:03:33.760
creates efficiency problems and it may not work well on test data.

68
00:03:33.760 --> 00:03:35.900
It may not generalize.

69
00:03:35.900 --> 00:03:39.910
So here is a strategy which turns out to be extremely powerful.

70
00:03:39.910 --> 00:03:43.360
I'm going to put together a procedure that involves a great deal of

71
00:03:43.360 --> 00:03:47.290
randomness and we'll build quite a good tree.

72
00:03:47.290 --> 00:03:49.840
Quite good in a sense that we don't need to discuss.

73
00:03:49.840 --> 00:03:52.920
It will be moderately accurate and shallow.

74
00:03:52.920 --> 00:03:57.955
So for example I could take the extreme step of using only two leaves,

75
00:03:57.955 --> 00:03:59.350
meaning there's exactly one decision.

76
00:03:59.350 --> 00:04:01.150
If you're on one side, you're one label if,

77
00:04:01.150 --> 00:04:03.250
you're on the other side the other label.

78
00:04:03.250 --> 00:04:07.930
That is by rather gross analogy called a decision stump.

79
00:04:07.930 --> 00:04:10.000
Now what I'm going to do is make lots of

80
00:04:10.000 --> 00:04:14.410
these quite good trees and that object I will think of as a random forest.

81
00:04:14.410 --> 00:04:17.530
And I can make them on the same training data set and still be

82
00:04:17.530 --> 00:04:20.550
sure that they're different because I'm making them with random aspects.

83
00:04:20.550 --> 00:04:23.115
So the randomness matters.

84
00:04:23.115 --> 00:04:26.050
And then what I'm going to do is average out

85
00:04:26.050 --> 00:04:31.505
the randomness and the decisions to come up with a good or maybe even the best decision.

86
00:04:31.505 --> 00:04:35.630
So there are two questions. Firstly, how do I come up with a decent tree?

87
00:04:35.630 --> 00:04:39.020
And secondly, what do I do once I've got a forest?

88
00:04:39.020 --> 00:04:41.115
Now whenever a tree is involved,

89
00:04:41.115 --> 00:04:47.975
computer scientists tend to have fairly good and solid and kind of immediate intuitions.

90
00:04:47.975 --> 00:04:49.914
So, how do you make a tree?

91
00:04:49.914 --> 00:04:55.315
Pretty clearly, I'm going to recursively take the pool of training data,

92
00:04:55.315 --> 00:04:59.440
choose some split which tells the difference between left and right.

93
00:04:59.440 --> 00:05:01.840
And then, I've got a left pool and

94
00:05:01.840 --> 00:05:05.840
a right pool and I'm going to invoke this procedure on those pools.

95
00:05:05.840 --> 00:05:10.315
And I can do that recursively but I have to figure out when I'm going to stop.

96
00:05:10.315 --> 00:05:14.170
And I'm going to stop if all the data in the pool has the same label.

97
00:05:14.170 --> 00:05:17.170
Why? Because then there's no point in splitting it.

98
00:05:17.170 --> 00:05:19.343
If the tree is too deep.

99
00:05:19.343 --> 00:05:21.336
Why do I want to stop when the tree is too deep?

100
00:05:21.336 --> 00:05:22.700
Well, there are two problems if a tree is deep.

101
00:05:22.700 --> 00:05:26.955
One is, I may have a tree with an inconvenient number of leaves.

102
00:05:26.955 --> 00:05:30.080
The second is, as the tree gets deeper and deeper,

103
00:05:30.080 --> 00:05:35.570
the decisions that I make tend to be based on less and less data and as a result,

104
00:05:35.570 --> 00:05:39.065
may be less and less accurate and they probably won't work on test data,

105
00:05:39.065 --> 00:05:40.935
so I don't want to go too deep.

106
00:05:40.935 --> 00:05:45.530
And the the third reason I might stop is if the pool of data is too small.

107
00:05:45.530 --> 00:05:48.680
It is probably not a particularly good idea to figure out a split from

108
00:05:48.680 --> 00:05:52.130
two data items because you don't know what the rest of the data is going to do.

109
00:05:52.130 --> 00:05:54.590
So these are going to be my stopping criteria.

110
00:05:54.590 --> 00:05:57.270
How do I choose a split?

111
00:05:57.270 --> 00:06:00.045
I will have, as a convention,

112
00:06:00.045 --> 00:06:03.105
that the split is always chosen by

113
00:06:03.105 --> 00:06:06.860
testing a feature vector component against a threshold.

114
00:06:06.860 --> 00:06:10.450
So my data is described by feature vectors and labels.

115
00:06:10.450 --> 00:06:11.720
I look at the feature vector,

116
00:06:11.720 --> 00:06:15.000
I look at the earth component and I

117
00:06:15.000 --> 00:06:18.540
say is it bigger than or smaller than some fixed threshold?

118
00:06:18.540 --> 00:06:19.680
If it's bigger you go one way,

119
00:06:19.680 --> 00:06:21.860
if it's smaller you go the other way.

120
00:06:21.860 --> 00:06:25.620
Now it should be fairly clear that it's quite easy to

121
00:06:25.620 --> 00:06:29.790
choose the best threshold by search given I have the right component.

122
00:06:29.790 --> 00:06:34.620
What I'm going to do is come up with some figure of merit for the goodness of a split,

123
00:06:34.620 --> 00:06:38.220
then I will build a series of thresholds,

124
00:06:38.220 --> 00:06:46.365
possibly even a split between every possible meaningful split for that pool of data,

125
00:06:46.365 --> 00:06:48.570
and then I will search that set of thresholds

126
00:06:48.570 --> 00:06:51.715
and I'll take the one with the best figure of merit.

127
00:06:51.715 --> 00:06:55.910
We will talk about how to make that figure of merit in a second.

128
00:06:55.910 --> 00:06:59.745
The other question is which component of the feature vector should I test?

129
00:06:59.745 --> 00:07:04.080
And a standard and very good solution for this question is I will

130
00:07:04.080 --> 00:07:08.825
randomly choose a small subset of the components,

131
00:07:08.825 --> 00:07:14.280
typically about root D. So the root of the dimension,

132
00:07:14.280 --> 00:07:17.095
that's going to be about the size of the subset.

133
00:07:17.095 --> 00:07:19.110
So if I have a 100 dimensional feature,

134
00:07:19.110 --> 00:07:21.700
I'll choose 10 components at random, something like 10.

135
00:07:21.700 --> 00:07:23.940
Then what I will do is take each one in

136
00:07:23.940 --> 00:07:27.495
turn and search for each one for a good threshold.

137
00:07:27.495 --> 00:07:31.530
And I'll keep the figure of merit for the best threshold for that component and

138
00:07:31.530 --> 00:07:35.370
then I'll just go to the next one in my set of 10 or or root D,

139
00:07:35.370 --> 00:07:40.525
and I'll see which of those root D has the best threshold and I'll keep that.

140
00:07:40.525 --> 00:07:45.840
Now what is crucial here is we can put together a figure of merit for the best split.

141
00:07:45.840 --> 00:07:47.730
And you can see this easily from these drawing.

142
00:07:47.730 --> 00:07:50.775
So if you look on this side,

143
00:07:50.775 --> 00:07:53.765
this split is really very good.

144
00:07:53.765 --> 00:07:58.395
And the reason it's very good is once I know which side of this tree I'm on,

145
00:07:58.395 --> 00:08:00.975
left or right, gee, I got that right.

146
00:08:00.975 --> 00:08:02.875
Once I know which side of this tree I'm on,

147
00:08:02.875 --> 00:08:05.280
I actually know the label accurately.

148
00:08:05.280 --> 00:08:08.625
There's a very good chance that this will work for future data as well.

149
00:08:08.625 --> 00:08:10.105
So if I'm on the left side I get an o,

150
00:08:10.105 --> 00:08:12.510
if I'm on the right side again x.

151
00:08:12.510 --> 00:08:14.755
But if you look at this tree,

152
00:08:14.755 --> 00:08:16.500
or the same dataset,

153
00:08:16.500 --> 00:08:20.040
with this split, this split is completely unhelpful.

154
00:08:20.040 --> 00:08:22.765
If I'm on the bottom,

155
00:08:22.765 --> 00:08:24.843
I could have an o or an x,

156
00:08:24.843 --> 00:08:26.205
they are about the same frequency.

157
00:08:26.205 --> 00:08:27.395
If I'm on the top,

158
00:08:27.395 --> 00:08:29.225
I could have an o or an x.

159
00:08:29.225 --> 00:08:33.590
So what I would really like to do is to have a split such that,

160
00:08:33.590 --> 00:08:35.648
if you split the pool of data,

161
00:08:35.648 --> 00:08:41.758
it's really easy to tell which label you have depending on which side you're on.

162
00:08:41.758 --> 00:08:43.780
I can formalize that.

163
00:08:43.780 --> 00:08:46.220
So to formalize that, I need the notion of

164
00:08:46.220 --> 00:08:48.525
entropy and I will not develop it in detail here.

165
00:08:48.525 --> 00:08:51.980
I'll assume that those who understand it will follow me in detail,

166
00:08:51.980 --> 00:08:56.000
that those who don't but want to will read the notes and those who don't

167
00:08:56.000 --> 00:09:00.290
will flip through the video confident that somebody knows what to do about all this.

168
00:09:00.290 --> 00:09:05.255
The point here is that I can account for how much information is provided by a split.

169
00:09:05.255 --> 00:09:08.124
So if you look at these two splits,

170
00:09:08.124 --> 00:09:12.240
this one is actually better than that one.

171
00:09:12.240 --> 00:09:17.410
And the reason that one is better than two,

172
00:09:17.410 --> 00:09:19.155
if I use split one,

173
00:09:19.155 --> 00:09:20.580
on the left side,

174
00:09:20.580 --> 00:09:23.145
if I end up there, then I know the label is x.

175
00:09:23.145 --> 00:09:25.020
And on the right side if I end up there,

176
00:09:25.020 --> 00:09:29.860
then I know that the label is zero with probability two thirds, otherwise x.

177
00:09:29.860 --> 00:09:32.700
Reason two, if I, if I use split two,

178
00:09:32.700 --> 00:09:35.295
if I end up on the left side,

179
00:09:35.295 --> 00:09:38.550
I'm going to get an x with two thirds,

180
00:09:38.550 --> 00:09:43.055
but if I end up on the right side, it's still 50/50.

181
00:09:43.055 --> 00:09:46.805
So the question I have to worry about is how to account for this.

182
00:09:46.805 --> 00:09:48.820
And the way I'm going to account for this is I'm going to

183
00:09:48.820 --> 00:09:51.220
compute what's known as the information gain,

184
00:09:51.220 --> 00:09:55.570
which is the entropy of the label conditioned on having

185
00:09:55.570 --> 00:10:01.180
no split minus the entropy of the label conditioned on having a split.

186
00:10:01.180 --> 00:10:03.680
And I want this to be as big as possible.

187
00:10:03.680 --> 00:10:05.575
So if you're really into entropy,

188
00:10:05.575 --> 00:10:07.660
you'll follow me immediately.

189
00:10:07.660 --> 00:10:10.030
There's another side with some calculations for those who are not

190
00:10:10.030 --> 00:10:13.395
but want to see some more detail.

191
00:10:13.395 --> 00:10:17.680
But basically remember that the entropy of a label given information is

192
00:10:17.680 --> 00:10:22.450
how many bits I need to supply to know the true value.

193
00:10:22.450 --> 00:10:24.980
So we have a procedure for building a tree.

194
00:10:24.980 --> 00:10:26.295
So how do I build a forest?

195
00:10:26.295 --> 00:10:29.590
Well, I'm going to take a data set of the usual n pairs

196
00:10:29.590 --> 00:10:34.170
of Xi which is a feature vector and Yi a label.

197
00:10:34.170 --> 00:10:37.165
I'm going to separate the dataset and do a test in a training set.

198
00:10:37.165 --> 00:10:42.130
I'm going to train multiple distinct decision trees on the training set.

199
00:10:42.130 --> 00:10:43.920
Now these are going to be distinct.

200
00:10:43.920 --> 00:10:48.765
And the reason they're going to be distinct is each time I train a split,

201
00:10:48.765 --> 00:10:51.170
I'm going to use a random set of components.

202
00:10:51.170 --> 00:10:53.210
So every time I train a tree on this dataset,

203
00:10:53.210 --> 00:10:57.185
I'm going to get a different tree. There is an alternative.

204
00:10:57.185 --> 00:11:02.205
I could do this by allowing different trees to see different training sets,

205
00:11:02.205 --> 00:11:04.935
but keeping track of which trees saw which training item.

206
00:11:04.935 --> 00:11:06.305
Then when I test,

207
00:11:06.305 --> 00:11:08.915
if the tree has seen that training item,

208
00:11:08.915 --> 00:11:15.500
I will leave the training item out of evaluation for that tree alone.

209
00:11:15.500 --> 00:11:18.650
So how do I classify now I have a forest?

210
00:11:18.650 --> 00:11:24.205
I'm going to take test example x and I'm going to pass it down each tree of the forest.

211
00:11:24.205 --> 00:11:27.035
Now I have to choose a strategy.

212
00:11:27.035 --> 00:11:30.410
I could say every time the example arrives at a leaf,

213
00:11:30.410 --> 00:11:34.905
record one vote for the label that occurs most often at the leaf.

214
00:11:34.905 --> 00:11:38.070
Now I'm going to choose the label with the most votes.

215
00:11:38.070 --> 00:11:41.160
And this strategy is effective,

216
00:11:41.160 --> 00:11:42.650
but you might be worried about one thing,

217
00:11:42.650 --> 00:11:47.660
which is you could reasonably feel that when the example arrives

218
00:11:47.660 --> 00:11:53.630
at a leaf which is 59x and 41 0,

219
00:11:53.630 --> 00:11:57.530
you should think about that vote rather differently than when the example arrives

220
00:11:57.530 --> 00:12:02.850
at a leaf with a 1000 x's and no zeros in it.

221
00:12:02.850 --> 00:12:09.020
And you could take that into account by each time the example arrives at the leaf,

222
00:12:09.020 --> 00:12:13.162
you could record the n l votes for each of the labels that occur at that leaf,

223
00:12:13.162 --> 00:12:16.425
where n l is the number of times the label appears in the training data.

224
00:12:16.425 --> 00:12:18.560
Now you choose the label with the most votes.

225
00:12:18.560 --> 00:12:23.128
So what I'm doing now is waiting leaves that are really confident, somewhat higher.

226
00:12:23.128 --> 00:12:24.765
Both strategies seem to work.

227
00:12:24.765 --> 00:12:29.848
I'm not aware of any obvious evidence that one is better than the other.

228
00:12:29.848 --> 00:12:32.395
So here's a very simple example.

229
00:12:32.395 --> 00:12:33.825
The details are in the text.

230
00:12:33.825 --> 00:12:35.870
What I did was I took the heart data set,

231
00:12:35.870 --> 00:12:36.900
there are five labels, zero, one,

232
00:12:36.900 --> 00:12:38.860
two, three and four.

233
00:12:38.860 --> 00:12:45.045
We've seen this before when I talked about class confusion matrices and in fact,

234
00:12:45.045 --> 00:12:46.680
if you look at this class confusion matrix,

235
00:12:46.680 --> 00:12:48.755
you've seen the class confusion matrix before.

236
00:12:48.755 --> 00:12:51.255
I used the r random forest package,

237
00:12:51.255 --> 00:12:53.655
I fed the data into that package

238
00:12:53.655 --> 00:12:58.475
and I got a class confusion matrix that looks like one you've seen before.

239
00:12:58.475 --> 00:13:01.290
Now you may recall, when I talked about class confusion matrices,

240
00:13:01.290 --> 00:13:05.690
I said it's probably not a good idea to distinguish between one, two,

241
00:13:05.690 --> 00:13:08.190
three and four, because basically

242
00:13:08.190 --> 00:13:11.610
the decision tree can't do it or the random forest can't do it.

243
00:13:11.610 --> 00:13:14.740
So what I might do, is just squash labels one, two,

244
00:13:14.740 --> 00:13:19.860
three and four into one case which I'll call one and look at that.

245
00:13:19.860 --> 00:13:21.165
And if I do that,

246
00:13:21.165 --> 00:13:24.040
I get a classifier that's really very well-behaved.

247
00:13:24.040 --> 00:13:27.570
The class error for zero is 16%,

248
00:13:27.570 --> 00:13:29.920
the error for one is 22%,

249
00:13:29.920 --> 00:13:33.707
the overall accuracy is really rather high. Here's a summary.

250
00:13:33.707 --> 00:13:36.603
Random forests are straightforward to build and very effective.

251
00:13:36.603 --> 00:13:39.255
I have not given you a detailed account of an algorithm,

252
00:13:39.255 --> 00:13:41.850
largely because the small number of people who

253
00:13:41.850 --> 00:13:44.580
really need to build the algorithm will have to go a little bit further,

254
00:13:44.580 --> 00:13:46.750
everybody else is going to use a package.

255
00:13:46.750 --> 00:13:50.880
There are very good software implementations freely available.

256
00:13:50.880 --> 00:13:54.330
Random forests are extremely nice because they can predict any kind of label.

257
00:13:54.330 --> 00:13:56.040
It does not have to be binary classification.