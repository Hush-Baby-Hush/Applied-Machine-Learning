WEBVTT

1
00:00:00.000 --> 00:00:01.770
This is David Forsyth from

2
00:00:01.770 --> 00:00:05.900
the University of Illinois and I'm going to talk about stochastic gradient descent.

3
00:00:05.900 --> 00:00:10.420
What I want to do is to think about how I'm going to minimize that cost function.

4
00:00:10.420 --> 00:00:15.855
So, there is a very general recipe for minimizing functions.

5
00:00:15.855 --> 00:00:17.560
Assume you have a point u.

6
00:00:17.560 --> 00:00:19.710
Choose a direction p,

7
00:00:19.710 --> 00:00:23.890
and it's a good idea for that direction to go downhill for fairly obvious reasons,

8
00:00:23.890 --> 00:00:26.740
and then move a small amount in that direction,

9
00:00:26.740 --> 00:00:34.300
update my point by saying the ith u plus a little bit times that p is the i+1th,

10
00:00:34.300 --> 00:00:36.620
and then just iterate that.

11
00:00:36.620 --> 00:00:40.315
What is the direction? It's usually downhill.

12
00:00:40.315 --> 00:00:42.010
It's either the gradient,

13
00:00:42.010 --> 00:00:45.135
or if you remember Newton's method,

14
00:00:45.135 --> 00:00:49.345
it's a combination of the gradient and the Hessian. What is h?

15
00:00:49.345 --> 00:00:51.940
Well, one thing you could do is,

16
00:00:51.940 --> 00:00:54.010
if you have a p that looks promising,

17
00:00:54.010 --> 00:00:58.210
search along that p to find an h that is good.

18
00:00:58.210 --> 00:01:00.970
Typically by looking at the value where you end up and then

19
00:01:00.970 --> 00:01:04.280
choose that h by looking at the different values you've looked at.

20
00:01:04.280 --> 00:01:07.060
h is sometimes called the step length.

21
00:01:07.060 --> 00:01:09.760
Now the problem we have right now is this story actually

22
00:01:09.760 --> 00:01:13.830
doesn't work for the problem we have to solve.

23
00:01:13.830 --> 00:01:15.220
Why?

24
00:01:15.220 --> 00:01:17.930
Well, let's look at the cost function.

25
00:01:17.930 --> 00:01:22.725
The cost function is an average of

26
00:01:22.725 --> 00:01:29.225
hinge losses over examples plus this regularization term.

27
00:01:29.225 --> 00:01:33.480
Now this is long and annoying as an expression.

28
00:01:33.480 --> 00:01:35.170
I want it to be shorter to make the point,

29
00:01:35.170 --> 00:01:41.240
so I'm going to write all these terms with a g. My average is an average of gi's,

30
00:01:41.240 --> 00:01:44.055
and then there's a g0 which corresponds to this a.

31
00:01:44.055 --> 00:01:45.960
That's enough to get me where I want to be.

32
00:01:45.960 --> 00:01:47.760
Now think about the gradient.

33
00:01:47.760 --> 00:01:52.755
The way you take the gradient is you average the gradients of each of the gi's,

34
00:01:52.755 --> 00:01:55.390
and then you add the gradient to g0.

35
00:01:55.390 --> 00:01:57.885
That sounds hunky-dory, and it is,

36
00:01:57.885 --> 00:01:59.805
unless you've got 100 million gis,

37
00:01:59.805 --> 00:02:02.160
at which point it's kind of annoying.

38
00:02:02.160 --> 00:02:10.355
So the problem we have to deal with is this n might be enormous.

39
00:02:10.355 --> 00:02:13.895
100 million is not outrageous here,

40
00:02:13.895 --> 00:02:16.430
and in the exercises we'll be dealing with

41
00:02:16.430 --> 00:02:19.755
hundreds of thousands as sort of a routine exercise.

42
00:02:19.755 --> 00:02:21.440
Now, what are we going to do about this?

43
00:02:21.440 --> 00:02:29.060
I do not want to average the gradient at 100 million examples every time I take a step.

44
00:02:29.060 --> 00:02:34.575
So, notice that that term is actually a population mean.

45
00:02:34.575 --> 00:02:38.610
I'm averaging over 100 million the gradient in each example.

46
00:02:38.610 --> 00:02:41.670
How do we deal with population means when we can't go and look at everything?

47
00:02:41.670 --> 00:02:45.350
Well, we draw a sample and we estimate it with a sample mean.

48
00:02:45.350 --> 00:02:50.495
So, what I'm going to do is draw uniformly and at random usually without replacement,

49
00:02:50.495 --> 00:02:54.005
but n is so big that it really doesn't matter.

50
00:02:54.005 --> 00:02:56.505
A sample of Nb.

51
00:02:56.505 --> 00:02:59.405
Nb is a batch size.

52
00:02:59.405 --> 00:03:03.515
Typically we choose this batch size because it makes the cache work best,

53
00:03:03.515 --> 00:03:05.205
or the disk seek work best,

54
00:03:05.205 --> 00:03:07.495
or the GPU happy or something like that.

55
00:03:07.495 --> 00:03:10.050
It's a number chosen for practical reasons.

56
00:03:10.050 --> 00:03:15.060
I'm going to choose Nb examples and I'm just going to compute their gradients.

57
00:03:15.060 --> 00:03:19.840
I'm going to average over those and I'll say that's the direction I'm going to move in.

58
00:03:19.840 --> 00:03:25.095
Now, my update is going to be I had some point,

59
00:03:25.095 --> 00:03:28.545
u is now a and b stacked together.

60
00:03:28.545 --> 00:03:33.545
Why? Because I like writing one equation for a and one equation for b. I've got

61
00:03:33.545 --> 00:03:40.295
my new u is the old one plus some amount times the search direction,

62
00:03:40.295 --> 00:03:48.635
and my search direction is going to be backwards along a sample mean of gradients,

63
00:03:48.635 --> 00:03:53.295
plus the gradient with the penalty.

64
00:03:53.295 --> 00:03:55.710
Why is the sample mean a sensible thing to do?

65
00:03:55.710 --> 00:03:58.345
Well, firstly you should notice that it's random.

66
00:03:58.345 --> 00:03:59.585
So, this vector is random,

67
00:03:59.585 --> 00:04:03.365
but I'm not just barging around randomly in

68
00:04:03.365 --> 00:04:09.025
u space because the expected value of this is the right thing.

69
00:04:09.025 --> 00:04:13.100
If you can't prove that instantly to yourself as we speak,

70
00:04:13.100 --> 00:04:14.365
stop the movie at this point,

71
00:04:14.365 --> 00:04:16.250
take out a pencil and a piece of paper and prove it.

72
00:04:16.250 --> 00:04:20.900
It's pretty obvious, so I'm not going to do it.

73
00:04:20.900 --> 00:04:23.040
Now the other thing you have to notice is that searching for

74
00:04:23.040 --> 00:04:25.400
a step length is a very bad idea as well.

75
00:04:25.400 --> 00:04:30.215
Why? Because to know if I've taken a big enough step,

76
00:04:30.215 --> 00:04:32.315
I have to evaluate the cost function.

77
00:04:32.315 --> 00:04:34.395
To evaluate the cost function,

78
00:04:34.395 --> 00:04:36.485
I have to average 100 million examples,

79
00:04:36.485 --> 00:04:38.210
and I don't feel like doing that because it might

80
00:04:38.210 --> 00:04:41.300
take weeks. So, I'm not going to do that.

81
00:04:41.300 --> 00:04:43.850
Instead, I'm simply going to say take

82
00:04:43.850 --> 00:04:48.080
large steps at the start because you need to make big changes,

83
00:04:48.080 --> 00:04:50.915
and take small steps near the end.

84
00:04:50.915 --> 00:04:56.630
So, eta will start being big and get smaller and smaller. How do I choose that?

85
00:04:56.630 --> 00:05:04.460
Well, I break up my training into epochs or seasons or whatever.

86
00:05:04.460 --> 00:05:09.110
So, I do a whole bunch of rounds of training and then I update at counter e.

87
00:05:09.110 --> 00:05:13.960
My step length for my new round of training is going to

88
00:05:13.960 --> 00:05:18.540
look like a constant over that counter plus another constant.

89
00:05:18.540 --> 00:05:22.025
Now, you should notice that this expression has some nice features.

90
00:05:22.025 --> 00:05:25.195
One is when e is small, this number is big.

91
00:05:25.195 --> 00:05:27.075
When e is big,

92
00:05:27.075 --> 00:05:30.585
eta is small, which is sort of what you want.

93
00:05:30.585 --> 00:05:35.215
The rules here are that eta should limit to zero eventually,

94
00:05:35.215 --> 00:05:39.560
and also that the sum of all etas should limit to infinity.

95
00:05:39.560 --> 00:05:42.830
It turns out that this rule will have that property,

96
00:05:42.830 --> 00:05:44.815
but there are other ones as well.

97
00:05:44.815 --> 00:05:47.612
A sensible question here at this point is how do I choose m and n.

98
00:05:47.612 --> 00:05:53.180
The usual thing is you try a couple of rounds of training and see what happens.

99
00:05:53.180 --> 00:05:56.235
Another thing you should notice over here is I cannot diagnose convergence.

100
00:05:56.235 --> 00:05:59.865
I can't tell when my optimization has finished.

101
00:05:59.865 --> 00:06:03.020
The reason I can't tell when my optimization is finished is that doing

102
00:06:03.020 --> 00:06:07.185
so would involve evaluating the cost function and that's too expansive.

103
00:06:07.185 --> 00:06:10.955
So usually what I do is a fixed number of steps,

104
00:06:10.955 --> 00:06:15.155
or possibly what I can do is hold out some examples.

105
00:06:15.155 --> 00:06:22.810
For them, build a little plot that looks like this: number of steps,

106
00:06:22.810 --> 00:06:26.275
error on held out,

107
00:06:26.275 --> 00:06:30.240
and that little part is going to wiggle a bit because I'm

108
00:06:30.240 --> 00:06:33.980
taking somewhat random steps, but I hope it goes down.

109
00:06:33.980 --> 00:06:35.245
I sit there and I'll watch it.

110
00:06:35.245 --> 00:06:39.500
If it gets small enough and I'm happy with where it is, I stop it.

111
00:06:39.500 --> 00:06:43.000
This algorithm is known as stochastic gradient descent.

112
00:06:43.000 --> 00:06:48.000
It's an extremely powerful algorithmic recipe. We will use again.